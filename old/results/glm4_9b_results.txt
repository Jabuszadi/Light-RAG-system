==================== Testy dla modelu LLM: glm4:9b ====================


==================== Pytanie: Kim jest Sam Altman? ====================

  [DEBUG] Kolekcja: all_minilm_collection
  [DEBUG] Odzyskane fragmenty (źródła):
    --- Fragment 1 (score: 0.2198, z pliku: llama.txt) ---
    # Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)...
--------------------------------------------------
    --- Fragment 2 (score: 0.2114, z pliku: mistal.txt) ---
    # Modele Mistral – kompendium (stan na sierpień 2025)...
--------------------------------------------------
    --- Fragment 3 (score: 0.2016, z pliku: pllum.txt) ---
    # PLLuM – polski model językowy...
--------------------------------------------------
    --- Fragment 4 (score: 0.1983, z pliku: mistal.txt) ---
    ## Opisy poszczególnych modeli...
--------------------------------------------------
    --- Fragment 5 (score: 0.1949, z pliku: mistal.txt) ---
    ### Mistral Saba
Mistral Saba to 24‑miliardowy model językowy zaprojektowany na potrzeby Bliskiego Wschodu i Azji Południowej. Artykuł z lutego 2025 r. opisuje go jako model szkolony na starannie dobranych danych arabskich i językach indyjskichhttps://mistral.ai/news/mistral-saba#:~:text=Mistral%20Saba%20is%20a%2024B,serve%20as%20a%20strong%20base.  Saba obsługuje języki takie jak arabski i tamil, zapewniając bardziej naturalne odpowiedzi niż modele kilkakrotnie większehttps://mistral.ai/news/mi...
--------------------------------------------------

  [DEBUG] Odpowiedź LLM (streaming): 

--- Kolekcja: all_minilm_collection ---
Odpowiedź: Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.
Czas przetwarzania: 9.32 sekund.


==================== Pytanie: Jakie modele LLaMa są dostępne? ====================

  [DEBUG] Kolekcja: all_minilm_collection
  [DEBUG] Odzyskane fragmenty (źródła):
    --- Fragment 1 (score: 0.4408, z pliku: llama.txt) ---
    # Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)...
--------------------------------------------------
    --- Fragment 2 (score: 0.3820, z pliku: llama.txt) ---
    ## LLaMA 2 (lipiec 2023)

LLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%...
--------------------------------------------------
    --- Fragment 3 (score: 0.3507, z pliku: llama.txt) ---
    ## Wprowadzenie

**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotó...
--------------------------------------------------
    --- Fragment 4 (score: 0.3385, z pliku: llama.txt) ---
    ## LLaMA 3.2 (wrzesień 2024)

Ta aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  

* **Sprzęt** – artykuł Hugging ...
--------------------------------------------------
    --- Fragment 5 (score: 0.3318, z pliku: mistal.txt) ---
    ### Mistral Embed
Mistral Embed to model embeddingowy dla tekstu (8 k kontekstu) dostępny jako usługa API. Został zaprojektowany do generowania gęstych reprezentacji semantycznych dla RAG i wyszukiwania. Chociaż Mistral nie opublikował dedykowanego bloga, dokumentacja API opisuje model jako dostarczający wektory 1024‑elementowe; usługa jest wykorzystywana w platformach takich jak Pinecone, Zilliz czy LangChain....
--------------------------------------------------

  [DEBUG] Odpowiedź LLM (streaming): 

--- Kolekcja: all_minilm_collection ---
Odpowiedź: 1. Modele LLaMa dostępne to:
   - **LLaMA 2**: modele 7 B, 13 B i 70 B.
   - **LLaMA 3.2**: modele 1 B, 3 B (tekstowe), 11 B Vision i 90 B Vision.

Te informacje są poparte następującymi fragmentami:
- LLaMA 2: "LLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**." (fragment z LLaMA 2)
- LLaMA 3.2: "Ta aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**." (fragment z LLaMA 3.2)
Czas przetwarzania: 3.31 sekund.


==================== Pytanie: Kto stworzył PLLuM? ====================

  [DEBUG] Kolekcja: all_minilm_collection
  [DEBUG] Odzyskane fragmenty (źródła):
    --- Fragment 1 (score: 0.4064, z pliku: pllum.txt) ---
    ## Tło i motywacja

PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. ...
--------------------------------------------------
    --- Fragment 2 (score: 0.3447, z pliku: pllum.txt) ---
    # PLLuM – polski model językowy...
--------------------------------------------------
    --- Fragment 3 (score: 0.3427, z pliku: pllum.txt) ---
    ## Podsumowanie

PLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,c...
--------------------------------------------------
    --- Fragment 4 (score: 0.3337, z pliku: pllum.txt) ---
    ## Zbiory danych i fine‑tuning

- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  
- **Instrukcje „organiczne”** – stworzono największy polski zes...
--------------------------------------------------
    --- Fragment 5 (score: 0.3329, z pliku: pllum.txt) ---
    ## Zastosowania

PLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:

- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.
- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykow...
--------------------------------------------------

  [DEBUG] Odpowiedź LLM (streaming): 

--- Kolekcja: all_minilm_collection ---
Odpowiedź: PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską. Wspierają go m.in. NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.
Czas przetwarzania: 1.72 sekund.


==================== Pytanie: Jaki model najlepiej działa na GPU z 24 GB VRAM? ====================

  [DEBUG] Kolekcja: all_minilm_collection
  [DEBUG] Odzyskane fragmenty (źródła):
    --- Fragment 1 (score: 0.5050, z pliku: llama.txt) ---
    ## Podsumowanie tabelaryczne

| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |
|---|---|---|---|---|---|
| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20...
--------------------------------------------------
    --- Fragment 2 (score: 0.4779, z pliku: llama.txt) ---
    ## LLaMA 1 (2023)

LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  

* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-r...
--------------------------------------------------
    --- Fragment 3 (score: 0.4763, z pliku: mistal.txt) ---
    ### Mistral 7B
Pierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99...
--------------------------------------------------
    --- Fragment 4 (score: 0.4261, z pliku: llama.txt) ---
    ### Code Llama (sierpień 2023)

Code Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  

* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements...
--------------------------------------------------
    --- Fragment 5 (score: 0.4196, z pliku: mistal.txt) ---
    ## Tabela 1 – skrócone parametry i wymagania VRAM
Poniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mog...
--------------------------------------------------

  [DEBUG] Odpowiedź LLM (streaming): 

--- Kolekcja: all_minilm_collection ---
Odpowiedź: 1. Odpowiedź: **Mistral Small 24B**, **Devstral Small 24B** i **Voxtral Mini 3B**.
   - Mistral Small 24B wymaga 60 GB VRAM, ale oferuje różnorodne funkcje, takie jak lokalni asystenci, obsługa funkcji i wizji.
   - Devstral Small 24B również wymaga 60 GB VRAM i jest używany jako agent kodeksowy do automatycznych poprawek kodu.
   - Voxtral Mini 3B wymaga 8 GB VRAM i jest kompaktowym modelem audio do edge-device, co czyni go odpowiednim dla starszych GPU z mniejszą ilością VRAM.
Czas przetwarzania: 4.22 sekund.


==================== Pytanie: Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę ====================

  [DEBUG] Kolekcja: all_minilm_collection
  [DEBUG] Odzyskane fragmenty (źródła):
    --- Fragment 1 (score: 0.4548, z pliku: llama.txt) ---
    ## Wnioski i rekomendacje

1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.
2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie j...
--------------------------------------------------
    --- Fragment 2 (score: 0.4405, z pliku: pllum.txt) ---
    ## Dostępne modele PLLuM

Tabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – t...
--------------------------------------------------
    --- Fragment 3 (score: 0.4325, z pliku: pllum.txt) ---
    ## Podsumowanie

PLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,c...
--------------------------------------------------
    --- Fragment 4 (score: 0.4270, z pliku: llama.txt) ---
    ## LLaMA 3.2 (wrzesień 2024)

Ta aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  

* **Sprzęt** – artykuł Hugging ...
--------------------------------------------------
    --- Fragment 5 (score: 0.4150, z pliku: llama.txt) ---
    ## LLaMA 3 (kwiecień 2024)

Trzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  

* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  
* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji...
--------------------------------------------------

  [DEBUG] Odpowiedź LLM (streaming): 

--- Kolekcja: all_minilm_collection ---
Odpowiedź: 1. Aby odpalić najmniejszy model LLaMA, który jest **LLaMA 3.2 (1B)**, potrzeba około **0,75 GB VRAM** w trybie INT4.

Poprawienie:
- W kontekście dotyczącym LLaMA 3.2 zwrócono uwagę na to, że model 1B wymaga **0,75 GB** VRAM w trybie INT4 (źródło: [PLLuM – polski model językowy/](https://github.com/CYFRAGOVPL/rag_project/blob/master/docs/pllum.txt)).
Czas przetwarzania: 3.44 sekund.
