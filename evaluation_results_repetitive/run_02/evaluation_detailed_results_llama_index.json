{"question": "Kim jest Sam Altman?", "llm_model_name": "qwen3:8b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking who Sam Altman is, and they want the answer based solely on the provided context. First, I need to scan through the context information given.\n\nLooking at the documents, there's a lot about different AI models like GPT, Mistral, and others. The GPT section mentions OpenAI and their models, but I don't see any mention of Sam Altman there. The Mistral AI documents talk about the company and their models, but again, no reference to Sam Altman. \n\nWait, the user provided a context that includes a file path pointing to \"gpt.txt\" and \"mistal.txt\", but the actual content of those files isn't included here. The context given is the headers and some tables about the models, but no biographical information on Sam Altman. \n\nSince the context doesn't mention Sam Altman at all, the answer should state that the information isn't found in the provided documents. The user's instructions say to respond with a specific message if the answer isn't in the context. So, the correct response here is to apologize and mention that the documents don't contain the answer.\n</think>\n\nPrzepraszamy, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "### Mistral Small 3, 3.1 i 3.2 (24 B parametrów)\nSeria Mistral Small to 24‑miliardowe modele zoptymalizowane pod kątem niskiej latencji. Wersja 3 z stycznia 2025 r. oferuje 81 % accuracy na MMLU i przepływ 150 tokenów/s; model został zaprojektowany tak, aby zmieścić się na pojedynczej karcie RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3#:~:text=Mistral%20Small%203%20is%20a,performance%2C%20with%20very%20low%20latencyhttps://mistral.ai/news/mistral-small-3#:~:text=,a%20Macbook%20with%2032GB%20RAM. Wersja 3.1 (marzec 2025) dodaje możliwość obsługi obrazów, funkcji i kontekstu 128k; nadal można ją uruchamiać na RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3-1#:~:text=performance%2C%20multimodal%20understanding%2C%20and%20an,of%20150%20tokens%20per%20second.  Wersja 3.2 (czerwiec 2025) to aktualizacja skupiająca się na lepszym przestrzeganiu instrukcji, redukcji powtórzeń i solidniejszym wywoływaniu funkcji – wewnętrzne miary wzrosły z 82,75 % do 84,78 % na testach instruction‑followinghttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks; zewnętrzne testy Wildbench i Arena Hard znacznie się poprawiłyhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks.  3.2 wymaga ~55 GB VRAM w bf16, a dzięki kwantyzacji może działać na kartach z 32 GB RAMhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=Mistral%20Small%203,serve%20access%20and%20direct%20deployment.  Modele z serii Small są dostępne w licencji Apache 2.0 i stanowią fundament do fine‑tuningów, asystentów lokalnych, RAG oraz zadań multimodalnych.", "### Magistral Small (24 B) i Magistral Medium\nMagistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r. Blog Mistral AI opisuje ją jako dwa warianty: Magistral Small (24 B) udostępniony na licencji Apache 2.0 oraz Magistral Medium (wersja korporacyjna)https://mistral.ai/news/magistral#:~:text=Today%2C%20we%E2%80%99re%20excited%20to%20announce,along%20with%20deep%20multilingual%20flexibility. Modele zostały zaprojektowane z myślą o transparentnym łańcuchu rozumowania (chain‑of‑thought), wielojęzyczności i obsłudze domen specjalistycznych.  Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %https://mistral.ai/news/magistral#:~:text=,a%20more%20powerful%2C%20enterprise%20version.  Magistral Small ma kontekst 40k i minimalny wymóg 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,2507%2024B%2024B%2060; może być pobrany z HuggingFace i samodzielnie wdrożonyhttps://mistral.ai/news/magistral#:~:text=Availability. Magistral Medium jest dostępny poprzez API i dla klientów chmurowych; Mistral podkreśla szybkość (Flash Answers w Le Chat) oraz zastosowania w prawie, finansach i regulacjachhttps://mistral.ai/news/magistral#:~:text=With%20Flash%20Answers%20in%20Le,and%20user%20feedback%2C%20at%20scale.", "### Mistral Medium 3\nModel Mistral Medium 3 (maj 2025) jest pierwszym „średnim” modelem w ofercie Mistral AI. Artykuł „Medium is the new large” podkreśla, że model zapewnia wydajność zbliżoną do modeli premium, przy ośmiokrotnie niższym koszcie i upro­sz­czonym wdrożeniuhttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20state,with%20radically%20simplified%20enterprise%20deployments. Medium 3 jest dostępny w formie API; można go także wdrożyć on‑premises na zestawie czterech GPUhttps://mistral.ai/news/mistral-medium-3#:~:text=Additionally%2C%20Mistral%20Medium%203%20can,of%20four%20GPUs%20and%20above. Model jest przeznaczony do zastosowań korporacyjnych: programowania, multimodalnej analizy i zadaniowych agentów; benchmarki pokazują, że osiąga ponad 90 % wyników Claude Sonnet 3.7 przy niższej ceniehttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20frontier,2%20output%20per%20M%20token.  Mistral Medium nie posiada publicznej tabeli wymagań VRAM; w praktyce wymaga wielu GPU.", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 15.341715812683105, "generated_answer_word_count": 202, "faithfulness_score": 0.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "qwen3:8b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking which LLaMA models are available, and they want the answer strictly based on the provided context. First, I need to go through the context information thoroughly.\n\nLooking at the table in the \"Podsumowanie tabelaryczne\" section, there's a list of models from LLaMA 1 to LLaMA 4, including various versions like 3.1, 3.2, 3.2-Vision, 4 Scout, 4 Maverick. Each entry has details like parameters, context window, VRAM requirements, licenses, and use cases. \n\nThen, in the \"Wnioski i rekomendacje\" section, there's a summary of models again, mentioning LLaMA 1, 2, 3, 3.1, 3.2, 3.2-Vision, 4 Scout, 4 Maverick. The \"Wprowadzenie\" section also mentions LLaMA 1, 2, 3, 3.2, 4. \n\nFurther down, each subsection under \"LLaMA 2\", \"LLaMA 1\", \"Code Llama\", \"LLaMA 3.2\", \"LLaMA 3.1\", \"LLaMA 3\" provides specific model details. For example, Code Llama is mentioned with 7B, 13B, 30B/34B. LLaMA 3.2 has 1B, 3B, and Vision variants. LLaMA 3.1 includes 8B, 70B, 405B. LLaMA 4 has Scout (17B active, 109B total) and Maverick (17B active, 400B total).\n\nI need to compile all these model names and their variants. The user wants the answer to be fully supported by the context, so I should list each model mentioned, ensuring not to add anything outside the provided data. Also, check for any possible duplicates or overlaps in the model names. For instance, LLaMA 3.1 and 3.2 are separate, and the Vision variants are part of 3.2. The Scout and Maverick are part of LLaMA 4. \n\nMake sure to include all the models mentioned in the context, such as LLaMA 1, 2, 3, 3.1, 3.2, 3.2-Vision, 4 Scout, 4 Maverick, and the Code Llama variants. Also, note the specific parameters and versions where applicable, but the user's question is just about availability, so maybe just the model names and their versions. However, the user might expect the parameters as part of the model names since the context specifies them. \n\nWait, the user's question is \"Jakie modele LLaMa są dostępne?\" which translates to \"Which LLaMA models are available?\" So the answer should list all the model names and their variants as per the context. The context includes models like LLaMA 1 (7B, 13B, 65B), LLaMA 2 (7B, 13B, 70B), Code Llama (7B, 13B, 30B/34B), LLaMA 3 (8B, 70B), LLaMA 3.1 (8B, 70B, 405B), LLaMA 3.2 (1B, 3B, 11B, 90B Vision), LLaMA 4 Scout (17B active, 109B total), LLaMA 4 Maverick (17B active, 400B total). \n\nI need to present all these models, ensuring each is mentioned with their specific parameters and versions as per the context. Also, check if any models are mentioned in multiple sections, like LLaMA 3.1 and 3.2, and include them all. The answer should be a comprehensive list of all available models based on the context provided.\n</think>\n\nOdpowiedź:  \nDostępne modele LLaMA obejmują:  \n1. **LLaMA 1** – wersje 7B, 13B, 65B (parametry gęste).  \n2. **LLaMA 2** – wersje 7B, 13B, 70B (parametry gęste).  \n3. **Code Llama** – wersje 7B, 13B, 30B, 34B (parametry gęste).  \n4. **LLaMA 3** – wersje 8B, 70B (parametry gęste).  \n5. **LLaMA 3.1** – wersje 8B, 70B, 405B (parametry gęste).  \n6. **LLaMA 3.2** – wersje 1B, 3B (tekstowe), 11B, 90B (multimodalne z obsługą obrazu).  \n7. **LLaMA 3.2-Vision** – wersje 11B, 90B (tekst + obraz).  \n8. **LLaMA 4 Scout** – 17B aktywnych / 109B całkowitych (MoE).  \n9. **LLaMA 4 Maverick** – 17B aktywnych / 400B całkowitych (MoE).  \n\nWszystkie modele są dostępne zgodnie z licencją Llama Community License, z wyjątkiem wersji multimodalnych 3.2 i 4, które mają dodatkowe ograniczenia w UE. Dostępne są również warianty z różnymi poziomami kwantyzacji (np. 4-bit, FP8, FP16), co wpływa na wymagania VRAM (np. 4–16 GB dla 8B, 35–140 GB dla 70B, 203–810 GB dla 405B).", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "# Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 40.68791961669922, "generated_answer_word_count": 620, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": 0.9375, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "<think>\nOkay, the user is asking who created PLLuM. Let me check the provided context.\n\nLooking through the documents, the \"Tło i motywacja\" section mentions that PLLuM is a project by a consortium of Polish universities and institutes, coordinated by Wrocław University of Technology and supported by several organizations like NASK PIB, PAN, and others. The article also states that the model was officially presented by the Ministry of Digitization. \n\nAnother part in the \"Podsumowanie\" section reiterates that PLLuM was developed by a consortium including OPiP (Ośrodek Przetwarzania Informacji PIB), Wrocław University of Technology, and other institutions. The \"Zalety i wyróżniki\" section mentions the Polish language model was developed by the consortium, with contributions from OPiP and others. \n\nSo, the key points are the consortium of Polish institutions, coordinated by Wrocław University of Technology, and supported by NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki, and Instytut Slawistyki PAN. The Ministry of Digitization is mentioned as the presenter, not necessarily the creator. \n\nI need to make sure not to include any external knowledge. The answer should strictly reference the consortium and the coordinating institutions listed in the context.\n</think>\n\nPLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską. W skład konsorcjum wchodzili m.in. NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN. Projekt został oficjalnie zaprezentowany przez Ministerstwo Cyfryzacji.", "retrieved_contexts": ["## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "# PLLuM – polski model językowy", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "# Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)"], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 24.91470241546631, "generated_answer_word_count": 234, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": 1.0, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "<think>\nOkay, I need to figure out which model works best on a GPU with 24 GB VRAM based on the provided context. Let me go through the documents carefully.\n\nFirst, looking at the Mistral models. The Mistral 7B requires 16 GB VRAM. The Mixtral 8x7B needs 100 GB, which is way over 24 GB. The Mixtral 8x22B needs 300 GB, so that's too much. The Mistral NeMo 12B is 28 GB in bf16 and 16 GB in fp8. Wait, fp8 is 16 GB, which is under 24. But the question is about the best model, so maybe the Mistral NeMo 12B in fp8 could fit. However, the context mentions that Mistral NeMo 12B has 28 GB in bf16 and 16 GB in fp8. So fp8 would fit into 24 GB. But is there a model that's better suited?\n\nLooking at the LLaMA models. The LLaMA 3.2 has a 3B model that in INT4 requires 1.75 GB VRAM. The 70B model in INT4 needs 35 GB VRAM, which is over 24. The LLaMA 3.1 70B in INT4 is 35 GB, still over. The LLaMA 3.1 8B in INT4 is 4 GB, which is way under. But maybe the LLaMA 3.2 3B model is a candidate. However, the context also mentions the Mistral Small 24B models. Wait, the Mistral Small 24B models have a VRAM requirement of 60 GB, which is over 24. \n\nWait, the Mistral NeMo 12B in fp8 is 16 GB, which fits into 24 GB. But the question is about the best model. The Mistral NeMo 12B is a larger model than the 7B, so it might perform better. However, the context also mentions the Pixtral 12B which requires 28 GB in bf16 and 16 GB in fp8. So Pixtral 12B in fp8 would also fit into 24 GB. \n\nWait, the Pixtral 12B's fp8 requirement is 16 GB, same as Mistral NeMo 12B. But the Mistral NeMo 12B is mentioned as a replacement for Mistral 7B. The Mistral NeMo 12B has a context up to 128k tokens, which is better than the 8k of Mistral 7B. The Pixtral 12B is multimodal, handling images and text, which might be a better fit if the task requires that. However, the question doesn't specify the task, just the VRAM. \n\nLooking at the GPT models, the GPT-OSS 20B in INT4 is 16 GB VRAM. That's exactly 16 GB, which is within 24 GB. The GPT-OSS 120B is 80 GB, which is too much. The GPT-OSS 20B would be a good candidate. \n\nBut wait, the GPT-OSS 20B is mentioned as requiring 16 GB VRAM in INT4. That's a very efficient model. However, the Mistral NeMo 12B and Pixtral 12B also fit into 24 GB. \n\nWait, the Mistral NeMo 12B in fp8 is 16 GB, which is under 24. The Pixtral 12B in fp8 is also 16 GB. The GPT-OSS 20B in INT4 is 16 GB. So all three models fit into 24 GB. \n\nBut the question is which one \"najlepiej\" (best) works. The context mentions that Mistral NeMo 12B is a replacement for Mistral 7B and has a larger context window (up to 128k tokens). The Pixtral 12B is multimodal, which might be better for certain tasks. The GPT-OSS 20B is a recent open-weight model, but the context says it's available in GPT-OSS, which is part of the GPT series. \n\nBut the user is asking for the best model for a 24 GB GPU. The Mistral NeMo 12B in fp8 is 16 GB, so there's 8 GB left, which could be useful. The GPT-OSS 20B is also 16 GB. The Pixtral 12B is similar. However, the Mistral NeMo 12B is noted as a replacement for Mistral 7B and has better performance. \n\nAlternatively, the Mistral Small 24B models require 60 GB, which is over. The Ministral 8B requires 24 GB in fp16. Wait, the Ministral 8B has a VRAM requirement of 24 GB in Mistral Commercial/Research License. That's exactly 24 GB. So the Ministral 8B would fit perfectly. \n\nWait, the Ministral 8B is mentioned with a VRAM requirement of 24 GB in fp16. The context says: \"Ministral 8B | 8 B | 128k tokens (32k vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on-device\". So that model requires exactly 24 GB VRAM. That's a perfect fit. \n\nSo comparing all options, the Ministral 8B is exactly 24 GB, which is the best fit. The Mistral NeMo 12B in fp8 is 16 GB, but maybe the Ministral 8B is better suited for tasks requiring lower latency and on-device use. \n\nTherefore, the answer should be the Ministral 8B.\n</think>\n\nOdpowiedź: Najlepiej działający model na GPU z 24 GB VRAM to **Ministral 8B**. Wymagania VRAM dla tego modelu wynoszą dokładnie 24 GB (w trybie fp16), co umożliwia jego uruchomienie bez przekraczania dostępnych zasobów. W kontekście dokumentu podano, że model ten jest przeznaczony do niskolatencyjnych asystentów on-device, co wskazuje na jego efektywność w takim konfiguracji sprzętowej.  \n\nDokładne źródło:  \n- **Ministral 8B**: \"Ministral 8B | 8 B | 128k tokens (32k vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on-device\".", "retrieved_contexts": ["## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "# Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 49.547797441482544, "generated_answer_word_count": 846, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "qwen3:8b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking for the amount of VRAM needed to run the smallest LLM model mentioned in the context, along with the model's name.\n\nFirst, I need to scan through the provided context to find all mentions of VRAM requirements and model sizes. The context includes several models from different generations like LLaMA 1, 2, 3, 3.1, 3.2, 4, Code Llama, and Mistral. Also, there's information about PLLuM models. \n\nStarting with LLaMA models: \n\n- LLaMA 1 has variants like 7B, 13B, 65B. The 7B model in FP16 requires 12-13 GB VRAM, but with 4-bit quantization, it's around 3-6 GB VRAM. However, the smallest model here is 7B, but the VRAM needed depends on quantization. However, the context mentions that the smallest model in some sections might be different.\n\nLooking at LLaMA 2: The 7B model in 4-bit quantization needs about 6 GB VRAM. That's lower than the 12-13 GB for FP16. But the user is asking for the smallest model, so maybe 7B is the smallest here.\n\nThen there's Code Llama: The 7B variant requires ~6 GB VRAM in 4-bit. But Code Llama is a variant of LLaMA 2, so maybe the 7B is the same as LLaMA 2's 7B.\n\nLooking further, in the LLaMA 3.2 section, there's a 1B model. The 1B model in FP16 requires 2.5 GB VRAM, FP8 1.25 GB, INT4 0.75 GB. That's smaller than the 7B models. Wait, but the user is asking for the smallest model. So the 1B model would be the smallest. But wait, the context mentions LLaMA 3.2 has 1B and 3B models. So the 1B model is smaller than the 7B models. \n\nBut wait, the user is asking for the \"najmniejszy model llm\" which translates to the smallest LLM model. So the 1B model would be the smallest. However, I need to check if there's any smaller model mentioned. \n\nLooking at the Mistral models in the context, the smallest is Mistral 7B, but that's 7B parameters. The PLLuM models have 8B, 12B, etc. So the 1B model in LLaMA 3.2 is the smallest. \n\nWait, but in the LLaMA 3.2 section, the 1B model is mentioned. Let me check the exact context. \n\nIn the section about LLaMA 3.2 (wrzesień 2024), it says: \"Modele 1 B i 3 B są trenowane na wielojęzycznym korpusie...\" and under the \"Sprzęt\" section, it states that the 1B model requires 2.5 GB VRAM in FP16. \n\nSo the smallest model mentioned is the LLaMA 3.2 1B model, which needs 2.5 GB VRAM in FP16. However, if quantization is applied, like INT4, it's 0.75 GB. But the question is about the VRAM needed to run the model. The context mentions that the 1B model in FP16 requires 2.5 GB VRAM. However, the user might be asking for the minimal VRAM required, which could be in a quantized form. But the question says \"najmniejszy model llm\" which refers to the model with the fewest parameters, which is 1B. \n\nBut wait, the LLaMA 3.2 1B model is mentioned, and the VRAM required for it is 2.5 GB (FP16). However, there's also the Mistral 3B model which requires 8 GB VRAM in FP16. But the 1B model is smaller in parameters. \n\nTherefore, the answer should be the LLaMA 3.2 1B model requiring 2.5 GB VRAM in FP16. However, the context also mentions the 1B model in the LLaMA 3.2 section. \n\nWait, but in the LLaMA 3.2 section, the 1B model is mentioned as requiring 2.5 GB VRAM in FP16, 1.25 GB in FP8, and 0.75 GB in INT4. So the minimal VRAM would be 0.75 GB with INT4 quantization. But the user is asking for the VRAM needed to run the model, which could vary based on quantization. However, the question is about the smallest model, which is the 1B model. \n\nBut the user's question is to find the smallest model in terms of parameters and the VRAM needed. The 1B model is the smallest in parameters. The VRAM required for it in FP16 is 2.5 GB. But if the user is asking for the minimal VRAM required, considering quantization, then it's 0.75 GB. However, the question doesn't specify the quantization level, so the answer should be based on the context's mention of the 1B model's VRAM requirements. \n\nAlternatively, maybe there's a smaller model. Wait, looking back, in the PLLuM models, there's a Llama-PLLuM-8B model, but that's 8B parameters. The 1B model is smaller. \n\nTherefore, the answer is the 1B model (LLaMA 3.2 1B) requiring 2.5 GB VRAM in FP16. However, the context also mentions that the 1B model can be run with 0.75 GB VRAM in INT4. But the user's question is about the VRAM needed to run the model, which could depend on the quantization. However, the question doesn't specify the quantization, so the answer should state the VRAM required for the model in its default or standard configuration. \n\nWait, the context for the 1B model says: \"model 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)\" so the VRAM needed depends on the quantization. The user is asking for the VRAM needed to run the model. If the question is about the minimal VRAM required, then it's 0.75 GB with INT4. However, the user might be asking for the VRAM needed without quantization, which is 2.5 GB. \n\nBut the question is to find the smallest model, which is the 1B model. The VRAM needed for that model would be the minimal required, which is 0.75 GB with INT4. However, the context might not explicitly state that the 1B model is the smallest. Wait, looking at all the models, the 1B model is the smallest in parameters. So the answer should be the LLaMA 3.2 1B model requiring 0.75 GB VRAM in INT4. \n\nBut the user's question is in Polish, and the answer must be in Polish. The context mentions the 1B model under LLaMA 3.2, and the VRAM for that model in INT4 is 0.75 GB. However, the user might be asking for the VRAM needed to run the model, which could be in any quantization. But the question is to find the VRAM needed for the smallest model. \n\nAlternatively, maybe the smallest model is the Mistral 3B, but that's 3B parameters, which is larger than 1B. So the 1B model is the smallest. Therefore, the answer is 0.75 GB VRAM for the 1B model in INT4. But the user might be looking for the VRAM needed in the default or standard configuration, which could be FP16. \n\nThe context for the 1B model states that it requires 2.5 GB VRAM in FP16, 1.25 GB in FP8, and 0.75 GB in INT4. The question is about the VRAM needed to run the model, so the answer should include the VRAM for the model, but the user might be asking for the minimal VRAM required. However, the question doesn't specify, so the answer should state the VRAM needed for the model in the most efficient quantization. \n\nBut since the user is asking for the smallest model, which is 1B parameters, and the VRAM needed for that model in INT4 is 0.75 GB, that's the answer. However, the context might not explicitly state that the 1B model is the smallest. Wait, in the LLaMA 3.2 section, it's mentioned as the 1B model, which is smaller than the 3B model. So the 1B model is the smallest in the context. \n\nTherefore, the answer is the LLaMA 3.2 1B model requiring 0.75 GB VRAM in INT4. However, the user's question might be looking for the VRAM in FP16, which is 2.5 GB. But the user didn't specify the quantization, so the answer should include the VRAM in the most efficient form. \n\nBut the context might not have any other model smaller than 1B. Therefore, the answer is the 1B model with 0.75 GB VRAM in INT4. However, the user's question might be looking for the minimal VRAM required, which would be the INT4 quantization. \n\nSo, the answer is the LLaMA 3.2 1B model requiring 0.75 GB VRAM in INT4.\n</think>\n\nOdpowiedź: Najmniejszy model LLm to **LLaMA 3.2 1B**, który wymaga **0,75 GB VRAM** przy użyciu kwantyzacji INT4.  \n\n**Podstawy:**  \n- W kontekście podano, że model **LLaMA 3.2 1B** wymaga **0,75 GB VRAM (INT4)**, co jest najmniejszym wymaganiem VRAM spośród wymienionych modeli.  \n- Brak mniejszych wariantów modeli LLm w kontekście, z wyjątkiem tego modelu.", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 61.227410078048706, "generated_answer_word_count": 1398, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 3.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "qwen3:8b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "<think>\nOkay, let's see. The user is asking who Sam Altman is, and they want the answer based solely on the provided context. I need to check all the documents to see if there's any mention of Sam Altman.\n\nLooking through the context, most of it talks about different AI models like Mistral, GPT, LLaMA, and PLLuM. There's a lot of technical details about their parameters, licenses, and usage. But I don't see any mention of Sam Altman. The documents discuss companies like Mistral AI, OpenAI, Meta, and others, but no reference to Sam Altman specifically. \n\nWait, maybe I missed something. Let me scan again. The GPT section mentions OpenAI and their models, but Sam Altman is the CEO of OpenAI, right? But the context here doesn't state that. The documents don't provide any information about the founders or executives of these companies. They focus on the models themselves, their features, and technical specs. \n\nSince there's no mention of Sam Altman in any of the provided fragments, the correct response is that the information isn't found in the context. The user emphasized using only the given documents, so I can't rely on external knowledge. Therefore, the answer should be the predefined message indicating the absence of the information.\n</think>\n\nPrzepraszamy, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 14.19581389427185, "generated_answer_word_count": 222, "faithfulness_score": 0.0, "answer_relevancy_score": null, "context_relevancy_score": 1.125, "correctness_score": 4.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "qwen3:8b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking which LLaMa models are available, and they want the answer strictly based on the provided context. First, I need to go through all the context information given.\n\nLooking at the sections about each version of LLaMA, there's a lot of detail. Starting with LLaMA 1, which has 7B, 13B, and 65B parameters. Then LLaMA 2 adds 7B, 13B, and 70B. LLaMA 3 includes 8B and 70B. LLaMA 3.1 expands to 8B, 70B, and 405B. LLaMA 3.2 introduces 1B, 3B, and Vision variants like 11B and 90B. LLaMA 4 has Scout and Maverick with different parameter counts. Also, there's mention of Code Llama with various sizes. \n\nI need to list all these models, making sure to include each variant and their parameters. The user wants precise numbers and any specific details like VRAM requirements or licensing, but the question is just about availability. Wait, the query is only about which models are available, so maybe focus on the model names and their parameter sizes. \n\nCheck each section again. For example, LLaMA 1: 7B, 13B, 65B. LLaMA 2: 7B, 13B, 70B. LLaMA 3: 8B, 70B. LLaMA 3.1: 8B, 70B, 405B. LLaMA 3.2: 1B, 3B, Vision 11B, 90B. LLaMA 4 Scout: 17B active, 109B total. Maverick: 17B active, 400B total. Code Llama: 7B, 13B, 30B, 34B. Also, the PLLuM model is mentioned but it's a separate Polish model, so maybe exclude that unless the user included it. Wait, the user's query is about LLaMa models, so PLLuM might be irrelevant here. \n\nMake sure to include all the models listed in the context. Each version's section lists the available models. Need to present them in order, specifying each variant and their parameter counts. Also, note any additional info like Vision models or Code Llama. The answer should be structured, listing each model with their parameters and any relevant details from the context. Avoid adding anything not in the context. Check for any missed models, like the Scout and Maverick in LLaMA 4. Also, the Code Llama variants. \n\nNow, compile the list, ensuring each model is mentioned with their respective parameter sizes and any specific notes like Vision or Code. The user wants the answer fully supported by the context, so each point must be directly from the provided text. No prior knowledge should be added. That's it.\n</think>\n\nOdpowiedź:  \nDostępne modele LLaMA obejmują:  \n1. **LLaMA 1**: 7B, 13B, 65B parametrów (gęste).  \n2. **LLaMA 2**: 7B, 13B, 70B parametrów (gęste).  \n3. **LLaMA 3**: 8B, 70B parametrów (gęste).  \n4. **LLaMA 3.1**: 8B, 70B, 405B parametrów (gęste).  \n5. **LLaMA 3.2**: 1B, 3B (tekstowe), 11B, 90B (multimodalne z obsługą obrazu).  \n6. **LLaMA 4**:  \n   - **Scout**: 17B aktywnych parametrów, 109B całkowitych (MoE).  \n   - **Maverick**: 17B aktywnych parametrów, 400B całkowitych (MoE).  \n7. **Code Llama**: 7B, 13B, 30B, 34B parametrów (gęste, specjalistyczne dla generacji kodu).  \n8. **LLaMA 3.2‑Vision**: 11B, 90B (multimodalne, obsługujące tekst i obraz).  \n\nWszystkie modele są objęte licencją Llama Community License (z wyjątkiem wersji multimodalnych w UE, które mają dodatkowe ograniczenia). Wymagania sprzętowe (np. VRAM) i zastosowania zostały opisane w kontekście.", "retrieved_contexts": ["## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 28.82074761390686, "generated_answer_word_count": 512, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": 1.0, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking who created PLLuM. I need to look through the provided context to find the answer.\n\nFirst, I'll scan the \"Tło i motywacja\" section. The first paragraph mentions that PLLuM is a project by a consortium of Polish universities and institutes, coordinated by Wrocław University of Technology and supported by several organizations like NASK PIB, IP PAN, etc. The URL provided also references an article about PLLuM. \n\nAnother mention in the \"Zalety i wyróżniki\" section under point 3 says the creators use licensed content and mention Polish copyright laws. The \"Podsumowanie\" section again refers to the consortium of Polish institutions. \n\nLooking at the \"Dostępne modele PLLuM\" table, the first entry for Llama-PLLuM-8B mentions it's based on Llama 3.1-8B, but that's about the base model, not the creators. \n\nSo, putting it all together, the consortium of Polish universities and institutes, coordinated by Wrocław University of Technology, are the creators. The specific institutions listed include NASK PIB, IP PAN, OPI PIB, University of Łódź, and PAN's Slavistics Institute. \n\nI need to make sure there's no other mention of creators elsewhere. The rest of the document talks about applications, features, datasets, and models, but not the creators. So the answer should list the consortium and the coordinating institution.\n</think>\n\nOdpowiedź: PLLuM został stworzony przez konsorcjum polskich uczelni i instytucji, koordynowane przez Politechnikę Wrocławską. Współtwórcami są m.in. NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN. Informacja ta pochodzi z sekcji „Tło i motywacja” oraz „Podsumowanie” dokumentu, które precyzyjnie wymienia instytucje zaangażowane w projekt.", "retrieved_contexts": ["## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "# PLLuM – polski model językowy"], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 18.030309677124023, "generated_answer_word_count": 266, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user wants to know which model works best on a GPU with 24 GB VRAM, based solely on the provided context.\n\nFirst, I need to go through the context information to find all mentions of models and their VRAM requirements. Let me start by scanning each section.\n\nLooking at the LLaMA models: \n\n- LLaMA 3.2 (3B) requires 3B: 6.5GB VRAM (FP16), 3.2GB (FP8), 1.75GB (INT4). So, 24GB is way more than needed here. But maybe there's a larger model that fits.\n- LLaMA 3.1 8B needs 16GB VRAM (FP16), 8GB (FP8), 4GB (INT4). So 24GB can handle that, especially in FP16.\n- LLaMA 3.2 Vision 11B (INT4) needs ~10GB VRAM. That's under 24GB. But the user might want the best performance, not just fitting.\n- Mistral models: Mistral 7B needs 16GB VRAM. Mistral NeMo 12B in FP8 requires 16GB. Mistral Small 24B in FP8 is 16GB. Wait, the user has 24GB, so maybe Mistral 7B or Mistral NeMo 12B in FP8 could fit. Also, Mixtral 8x7B in 4-bit needs 22.5GB VRAM, which is close to 24. That might be a candidate.\n- PLLuM models: PLLuM-12B in Q4_K_M requires 7.6GB VRAM. PLLuM-8x7B in 4-bit needs 22.5GB VRAM, which is under 24. So that's possible.\n\nNow, considering the best performance. The user might want the largest model that fits into 24GB. The Mistral 8x7B (45B parameters) in 4-bit uses 22.5GB VRAM, which is under 24. That's a large model. Also, LLaMA 3.1 8B in FP16 uses 16GB, but maybe with FP8 it's 8GB. But the Mistral 8x7B is a MoE model, which might offer better performance for certain tasks. Also, PLLuM-8x7B in 4-bit is 22.5GB, which is also a good fit. \n\nWait, the user's question is about \"najlepiej działa\" (best performance). The Mistral 8x7B is a MoE model, which can handle more complex tasks efficiently. Also, the LLaMA 3.2 3B in FP16 is 6.5GB, but that's a smaller model. The Mistral 7B requires 16GB, which is under 24, but the 8x7B is larger. \n\nSo, the Mistral 8x7B in 4-bit (22.5GB VRAM) and PLLuM-8x7B (22.5GB) are both candidates. However, the context mentions that Mistral 8x7B is a MoE model with 8 experts, which might provide better performance for complex tasks. Also, the PLLuM-8x7B is based on Mixtral 8x7B, so similar in structure. \n\nAnother option is Mistral NeMo 12B in FP8, which uses 16GB VRAM. That's also under 24. But the user might prefer a larger model. \n\nThe answer needs to be supported by the context. The key points are:\n\n- Mistral 8x7B (4-bit) requires 22.5GB VRAM.\n- PLLuM-8x7B (4-bit) requires 22.5GB VRAM.\n- Mistral 7B requires 16GB VRAM.\n- Mistral NeMo 12B (FP8) requires 16GB VRAM.\n- LLaMA 3.1 8B in FP16 is 16GB.\n\nThe Mistral 8x7B is a larger model (45B parameters) compared to LLaMA 3.1 8B (8B parameters). So, if the user wants the best performance, the Mistral 8x7B would be better. However, the context also mentions that PLLuM-8x7B is based on Mixtral 8x7B, so it's a Polish version. The user might prefer the Polish model if they need Polish language support. \n\nBut the question is about which model \"najlepiej działa\" (best works) on 24GB VRAM. The Mistral 8x7B in 4-bit is 22.5GB, which fits into 24GB. Similarly, PLLuM-8x7B also fits. However, the context mentions that Mistral 8x7B is a MoE model, which could offer better performance. \n\nSo, the answer should include both models, but the context might have specific mentions. Let me check the exact context again.\n\nIn the Mistral section, under \"Mistral 8×7B\", it says \"4‑bit kwantyzacja (~22,5 GB VRAM)\" which is under 24. Also, PLLuM-8×7B in the PLLuM section says \"Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~22,5 GB VRAM)\". So both models require 22.5GB. \n\nThe LLaMA 3.1 8B in FP16 is 16GB, which is also under 24, but the Mistral 8x7B is a larger model and might perform better. \n\nTherefore, the best models for 24GB VRAM are Mistral 8x7B (4-bit) and PLLuM-8x7B (4-bit), as they are the largest models that fit within the VRAM. However, if the user prefers a Polish model, PLLuM-8x7B would be the choice. But the question doesn't specify language. \n\nThe answer should list these models, citing the exact VRAM requirements from the context.\n</think>\n\nOdpowiedź:  \nNa GPU z 24 GB VRAM najlepiej działają modele **Mistral 8×7B** (4-bitowa kwantyzacja) oraz **PLLuM-8×7B** (4-bitowa kwantyzacja), które wymagają odpowiednio **22,5 GB VRAM** (patrz sekcje „Mixtral 8×7B” i „PLLuM-8×7B” w kontekście). Obie wersje są oparte na architekturze MoE (Mixture of Experts), co pozwala na efektywne wykorzystanie dostępnych zasobów. Dodatkowo, **LLaMA 3.1 8B** (FP16) wymaga **16 GB VRAM**, co również mieści się w 24 GB, ale oferuje mniejszy rozmiar parametrów w porównaniu do powyższych modeli. Wszystkie wymienione modele są wspierane przez dane z kontekstu.", "retrieved_contexts": ["## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "### Mistral NeMo 12B\nModel 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages.  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*"], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 41.29031777381897, "generated_answer_word_count": 794, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "qwen3:8b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking for the amount of VRAM needed to run the smallest LLM based on the provided context. They also want the model's name.\n\nFirst, I need to scan through the context documents to find mentions of the smallest model. Let me start by looking at the LLaMA series since that's a major part of the context. \n\nIn the LLaMA 3.2 section, there's a mention of models 1B and 3B. The 1B model is noted as the smallest. The VRAM requirements for the 1B model in INT4 quantization are 0.75 GB. That seems like the lowest VRAM requirement mentioned. \n\nWait, but there's also the Mistral models. The Mistral 3B model requires 8 GB VRAM in INT4. However, the LLaMA 1B model's requirement is lower. \n\nLooking at the PLLuM models, the smallest is the 8B version, which requires around 4.7 GB VRAM in Q4_K_S. But that's higher than the LLaMA 1B's 0.75 GB. \n\nAnother check: the LLaMA 3.2- Vision 11B model uses 10 GB VRAM in 4-bit quantization, which is higher. The Code Llama 7B model uses 6 GB VRAM in 4-bit. \n\nSo the smallest model in terms of VRAM is the LLaMA 1B model with 0.75 GB VRAM in INT4. The context mentions this under the LLaMA 3.2 section. \n\nI need to confirm that there's no smaller model mentioned elsewhere. The Mistral 3B is 3B parameters, which is larger than 1B. The PLLuM 8B is also larger. The LLaMA 1B is indeed the smallest here. \n\nTherefore, the answer should be the LLaMA 1B model requiring 0.75 GB VRAM in INT4 quantization. The context explicitly states this, so it's supported.\n</think>\n\nOdpowiedź:  \nNajmniejszy model LLM to **Llama 3.2-1B** (1 miliard parametrów), który wymaga **ok. 0,75 GB VRAM** w konfiguracji INT4.  \n\n**Podstawy z kontekstu:**  \n- W sekcji **LLaMA 3.2 (wrzesień 2024)** podano, że model **1B** wymaga **0,75 GB VRAM** w precyzji INT4 (zob.: *„model 1 B potrzebuje 2,5 GB (FP16), 1,25 GB (FP8) lub 0,75 GB (INT4)”*).  \n- Brak mniejszych modeli LLM w kontekście (np. PLLuM 8B wymaga 4,7 GB VRAM w Q4_K_S, a Mistral 3B – 8 GB VRAM w INT4).", "retrieved_contexts": ["## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 25.356569290161133, "generated_answer_word_count": 358, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": 1.0, "correctness_score": 4.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking if I'm an AI assistant whose only task is to answer questions using exclusively the provided context. The main question is about who Sam Altman is.\n\nFirst, I need to check the context provided. The context includes information about PLLuM, LLaMA models, their parameters, licensing, applications, and some technical details. However, I don't see any mention of Sam Altman in any of the documents. The context talks about various models, their developers, and technical specifications, but no reference to Sam Altman. \n\nThe user's instructions are clear: if the answer isn't in the context, I must respond that I can't find the information. Since there's no mention of Sam Altman in the provided fragments, the correct response is to state that the information isn't available in the context. I should avoid adding any external knowledge here. Let me double-check the context again to make sure I didn't miss any part. Nope, no mention of Sam Altman. So the answer should be the specified apology message.\n</think>\n\nPrzepraszamy, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 11.037104845046997, "generated_answer_word_count": 188, "faithfulness_score": 0.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking which LLaMa models are available, and they want the answer based solely on the provided context.\n\nFirst, I need to scan through the context documents to find mentions of LLaMa models. The context includes several files, but the relevant parts are in the 'llama.txt' file. \n\nLooking at the 'llama.txt' content, there's a section about LLaMA 3 under the \"LLaMA 3 (April 2024)\" heading. It mentions that the third generation includes models with 8B and 70B parameters. The text also talks about the architecture, hardware requirements, licensing, and applications of these models. \n\nAdditionally, there's a section titled \"Wnioski i rekomendacje\" (Conclusions and Recommendations) where it's stated that LLaMA offers models like 8B and 3B (LLaMA 3.2), but the main focus here is on LLaMA 3. The 70B model is mentioned in the context of requiring high VRAM and specific hardware. \n\nI should check if there are any other models mentioned. The 'pllum.txt' file discusses PLLuM models, which are based on LLaMA 3.1, Mistral Nemo, and Mixtral. However, the user is specifically asking about LLaMa models, not PLLuM. So, the PLLuM models are based on LLaMA but aren't LLaMa themselves. \n\nThe 'mistal.txt' file talks about Mistral models, which are separate from LLaMa. So I can ignore that. \n\nSo, the available LLaMa models according to the context are LLaMA 3 with 8B and 70B parameters. The 3B model is mentioned in the recommendations section as LLaMA 3.2, but the main ones highlighted are 8B and 70B. \n\nI need to make sure not to include any models from PLLuM or Mistral. Also, the user wants precise numbers and details from the context. The 8B and 70B are explicitly stated. The 3B is mentioned in the recommendations but might be part of a different version (LLaMA 3.2). However, the main answer should focus on the models directly discussed in the LLaMA sections.\n\nTherefore, the answer should list LLaMA 3 8B and 70B, referencing the context where they are mentioned. Also, note the licensing and hardware requirements if needed, but the user's question is specifically about which models are available. So the key point is the 8B and 70B models from LLaMA 3.\n</think>\n\nOdpowiedź:  \nDostępne modele LLaMa to:  \n- **LLaMA 3 8B** – model z 8 miliardami parametrów, który zapewnia wysoką jakość i niskie opóźnienia, szczególnie w zastosowaniach na urządzeniach końcowych.  \n- **LLaMA 3 70B** – model z 70 miliardami parametrów, oferujący zaawansowane możliwości w analizie tekstu i generacji, ale wymagający znacznego wyposażenia sprzętowego (np. ponad 140 GB VRAM w pełnej precyzji).  \n\nTe modele są omówione w sekcji „LLaMA 3 (April 2024)” oraz w „Wnioski i rekomendacje” dokumentu, gdzie podano ich parametry, wymagania sprzętowe i zastosowania.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 18.723952531814575, "generated_answer_word_count": 451, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking who created PLLuM. I need to look through the provided context to find the answer.\n\nFirst, I'll scan the sections related to the background and motivation of PLLuM. In the \"Tło i motywacja\" section, there's a mention that PLLuM is a project by a consortium of Polish universities and institutes, coordinated by Wrocław University of Technology and supported by several organizations like NASK PIB, PAN, etc. The link provided there points to a source that confirms the official presentation by the Ministry of Digitization. \n\nAnother part in the \"Podsumowanie\" section states that PLLuM was officially presented by the Ministry of Digitization on February 24, 2025, emphasizing linguistic accuracy and thematic diversity. The creators are highlighted as having developed the model based on ethically sourced data.\n\nI need to make sure there's no other mention of creators elsewhere. The \"Dostępne modele PLLuM\" section lists the models but doesn't mention the creators. The \"Zalety i wyróżniki\" section again refers to the consortium and the coordinating institutions. \n\nSo, putting it all together, the answer should include the consortium of Polish universities and institutes, specifically mentioning the coordinating institutions like Politechnika Wrocławska and the supporting organizations listed. The exact names from the context are crucial here. I must avoid adding any external information and strictly use the provided fragments.\n</think>\n\nOdpowiedź: PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowanego przez Politechnikę Wrocławską, z udziałem NASK PIB, Instytutu Podstaw Informatyki PAN, Ośrodka Przetwarzania Informacji PIB, Uniwersytetu Łódzkiego oraz Instytutu Slawistyki PAN. Twórcy podkreślają, że model został opracowany z wykorzystaniem etycznie pozyskanych danych i oferuje wersje przeznaczone do użytku komercyjnego oraz naukowego.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 13.959651947021484, "generated_answer_word_count": 278, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking which model works best on a GPU with 24 GB VRAM, and they want the answer based solely on the provided context. \n\nFirst, I need to go through the context documents to find information about VRAM requirements for different models. Let me recall the relevant sections.\n\nLooking at the LLaMA section, there's mention of Llama 3 models. The 8B model can run on a GPU with 16 GB VRAM, and the 70B requires much more. But the user has 24 GB, which is more than 16 GB. However, the 70B needs over 140 GB VRAM, so that's not suitable. The 8B model is a candidate here.\n\nThen there's the PLLuM models. The PLLuM-8B has a VRAM requirement of around 4.7 GB to 5.6 GB in different quantizations. The PLLuM-12B requires up to 12 GB VRAM for Mistral Nemo 12B, but with quantization, it can be lower. The PLLuM-8x7B (MoE) might need 22.5 GB VRAM in 4-bit quantization, which is close to 24 GB. Wait, the 8x7B model's 4-bit quantization needs about 22.5 GB, which is under 24 GB. That's a possibility.\n\nLooking at Mistral models, the Mistral 7B requires 16 GB VRAM in full precision. The Mixtral 8x7B needs 100 GB, which is too much. The Mistral Small 24B requires 60 GB, which is also over 24 GB. The Ministrals like 8B and 3B have lower requirements: 24 GB for 8B and 8 GB for 3B. Wait, the Ministrals 8B needs 24 GB VRAM, which matches the user's GPU. Also, the Mistral NeMo 12B can run with 16 GB VRAM in fp8, but that's lower than 24 GB. However, the user's GPU is 24 GB, so models that require up to 24 GB would fit.\n\nWait, the PLLuM-8x7B (MoE) in 4-bit quantization needs 22.5 GB VRAM, which is under 24 GB. That's a good candidate. Also, the Mistral 8B (Ministral 8B) requires 24 GB VRAM. Both models fit into 24 GB VRAM. \n\nBut the question is which model \"najlepiej działa\" (best works). The context mentions that the Mistral 8B (Ministral 8B) is designed for low-latency on-device assistants and has 128k token context. The PLLuM-8x7B is a MoE model with 8x7B parameters, which might offer better performance for certain tasks, but the VRAM requirement is slightly lower than 24 GB. However, the user's GPU is exactly 24 GB, so the Mistral 8B (Ministral 8B) would fit perfectly. Also, the PLLuM-8B model's quantized versions require less VRAM, but maybe the user wants the best performance, which might be the Mistral 8B. \n\nWait, the context also mentions that the Mistral 8B (Ministral 8B) has a VRAM requirement of 24 GB, which matches the user's GPU. The PLLuM-8x7B's 4-bit version needs 22.5 GB, which is under 24 GB, so it could also work. But the question is about which model works best. The answer should include both models if they fit, but the user might be looking for the one that's explicitly mentioned as fitting into 24 GB. \n\nLooking back, the Mistral 8B (Ministral 8B) requires exactly 24 GB VRAM. The PLLuM-8x7B's 4-bit version is 22.5 GB, which is under 24 GB. The user's GPU is 24 GB, so both models can run, but the Mistral 8B is specifically mentioned as requiring 24 GB. The answer should include both if they fit, but the context might have specific mentions. \n\nThe answer should list models that fit into 24 GB VRAM. The Mistral 8B (Ministral 8B) requires 24 GB. The PLLuM-8x7B (MoE) with 4-bit quantization needs 22.5 GB. Also, the PLLuM-8B in quantized versions (like Q4_K_S at 4.7 GB) is much lower, but maybe the user is looking for the best performance. However, the question is about which model works best on 24 GB VRAM. The answer should include both the Mistral 8B and the PLLuM-8x7B if they fit. \n\nWait, the context mentions that the Mistral 8B (Ministral 8B) requires 24 GB VRAM. The PLLuM-8x7B (MoE) with 4-bit quantization needs 22.5 GB. Both fit into 24 GB. The user's GPU is 24 GB, so both models can be run. However, the Mistral 8B is explicitly mentioned as requiring 24 GB, so it's a direct match. The PLLuM-8x7B is also a candidate. \n\nBut the user's question is about which model \"najlepiej działa\" (works best). The answer should list both models if they fit, but the context might have specific details. The Mistral 8B is mentioned as requiring 24 GB, which is exactly the user's GPU. The PLLuM-8x7B's 4-bit version uses 22.5 GB, which is under 2, so it's possible. However, the answer needs to be based on the context. The Mistral 8B is a direct match. Also, the PLLuM-8B in quantized versions would use less VRAM, but maybe the user wants the best performance. \n\nIn conclusion, the models that fit into 24 GB VRAM are the Mistral 8B (Ministral 8B) and the PLLuM-8x7B (MoE) with 4-bit quantization. The answer should include these models based on the context provided.\n</think>\n\nOdpowiedź:  \nModel **Mistral 8B (Ministral 8B)** najlepiej działa na GPU z 24 GB VRAM, ponieważ wymaga dokładnie 24 GB pamięci (zgodnie z informacją: *„Ministral 8B ... 24 GB VRAM”*). Dodatkowo, model **PLLuM-8×7B (MoE)** w wersji 4-bitowej wymaga ok. 22,5 GB VRAM (*„4-bitowa kwantyzacja (~22,5 GB VRAM)”*), co również pasuje do dostępnych zasobów. Obaj modele są optymalnie dopasowani do 24 GB VRAM, ale Mistral 8B jest jasno zdefiniowany jako wymagający dokładnie tej ilości, co sprawia, że jest najlepszym wyborem w kontekście dokładności danych.", "retrieved_contexts": ["## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## 3. Podsumowanie\n\nSeria GPT ewoluowała od niewielkiego modelu z 2018 r. do ogromnych systemów multimodalnych, które mogą przetwarzać milion tokenów i rozwiązywać złożone zadania.  W kontekście RAG kluczowe jest zrozumienie, że modele GPT > 3 są własnościowe i wymagają ogromnych zasobów; dlatego w praktycznych wdrożeniach, zwłaszcza on‑premise, lepiej korzystać z otwartych modeli takich jak **gpt‑oss**, Llama, Mistral lub innych LLM open‑source.  Jeśli planujesz użyć GPT‑3/4 w RAG, pamiętaj o limitach tokenów (4k–128k) oraz kosztach API; natomiast gpt‑oss zapewnia 128 k kontekstu i licencję Apache 2.0, co czyni go atrakcyjnym wyborem do długich dokumentów i rozbudowanych pipeline’ów.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 33.618680000305176, "generated_answer_word_count": 915, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": 1.0, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking about the memory requirements for the smallest LLM model, specifically the VRAM needed and the model's name. \n\nFirst, I need to look through the provided context to find information about the smallest model. The context mentions several models: Llama-PLLuM-8B, PLLuM-12B, PLLuM-70B, and PLLuM-8×7B. Among these, the 8B models are the smallest.\n\nLooking at the Llama-PLLuM-8B entry, it states that the model has 8 billion parameters and is based on Llama 3.1-8B. The VRAM requirements are mentioned as approximately 4.7 GB for Q4_K_S quantization. The context also notes that the full FP16 precision requires around 16.1 GB VRAM, but the user is asking for the minimal VRAM, which would be the quantized version.\n\nAnother section in the context discusses LLaMA 3 models. The Llama 3 8B model requires at least 16 GB VRAM for full precision, but with 4-bit quantization, it can run on a GPU with 24 GB VRAM. However, the user is asking about the smallest model, which might refer to the PLLuM-8B rather than the LLaMA 3 8B, since PLLuM-8B is part of the PLLuM family and the context specifically mentions it as a model with 8B parameters.\n\nWait, the context also mentions the Llama-PLLuM-8B's VRAM requirements as around 4.7 GB for Q4_K_S. That seems lower than the LLaMA 3 8B's 16 GB VRAM. So the smallest model in terms of VRAM would be the Llama-PLLuM-8B with 4.7 GB VRAM. However, I need to confirm if there's a model with even lower requirements. The PLLuM-8B is mentioned as the smallest in the family, and the LLaMA 3 8B is part of the LLaMA family, which is discussed separately. The user might be referring to the PLLuM models specifically, given the context is about PLLuM.\n\nSo, the answer should be the Llama-PLLuM-8B model requiring approximately 4.7 GB VRAM. The context explicitly states that for the Llama-PLLuM-8B, the Q4_K_S quantization needs about 4.7 GB VRAM. That's the minimal requirement mentioned.\n</think>\n\nOdpowiedź:  \nAby odpalić najmniejszy model, **Llama-PLLuM-8B (base/instruct/chat)**, wymagana jest **ok. 4,7 GB VRAM** przy użyciu kwantyzacji Q4_K_S. Informacja ta pochodzi z sekcji opisującej wymagania pamięci dla tego modelu, gdzie podano, że rozmiary kwantyzacji Q4_K_S odpowiadają minimalnym wymaganiom VRAM do uruchomienia modelu.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## 3. Podsumowanie\n\nSeria GPT ewoluowała od niewielkiego modelu z 2018 r. do ogromnych systemów multimodalnych, które mogą przetwarzać milion tokenów i rozwiązywać złożone zadania.  W kontekście RAG kluczowe jest zrozumienie, że modele GPT > 3 są własnościowe i wymagają ogromnych zasobów; dlatego w praktycznych wdrożeniach, zwłaszcza on‑premise, lepiej korzystać z otwartych modeli takich jak **gpt‑oss**, Llama, Mistral lub innych LLM open‑source.  Jeśli planujesz użyć GPT‑3/4 w RAG, pamiętaj o limitach tokenów (4k–128k) oraz kosztach API; natomiast gpt‑oss zapewnia 128 k kontekstu i licencję Apache 2.0, co czyni go atrakcyjnym wyborem do długich dokumentów i rozbudowanych pipeline’ów.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 17.247554779052734, "generated_answer_word_count": 372, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 3.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking if I can answer who Sam Altman is, using only the provided context. First, I need to scan through all the given documents to see if there's any mention of Sam Altman.\n\nLooking at the context, most of it discusses various AI models like LLaMA, Mistral, GPT, and PLLuM. There's detailed information about their features, licenses, hardware requirements, and applications. However, I don't see any mention of Sam Altman. The documents talk about companies like Meta, Mistral AI, OpenAI, and others, but not specifically about Sam Altman. \n\nThe user's instructions are clear: I must not use any prior knowledge and rely solely on the provided context. Since none of the fragments mention Sam Altman, the correct response is to state that the information isn't found in the documents. I need to make sure not to add any extra details or guess based on external knowledge. Therefore, the answer should be the predefined message indicating the inability to find the information.\n</think>\n\nPrzepraszamy, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "### Mistral Saba\nMistral Saba to 24‑miliardowy model językowy zaprojektowany na potrzeby Bliskiego Wschodu i Azji Południowej. Artykuł z lutego 2025 r. opisuje go jako model szkolony na starannie dobranych danych arabskich i językach indyjskichhttps://mistral.ai/news/mistral-saba#:~:text=Mistral%20Saba%20is%20a%2024B,serve%20as%20a%20strong%20base.  Saba obsługuje języki takie jak arabski i tamil, zapewniając bardziej naturalne odpowiedzi niż modele kilkakrotnie większehttps://mistral.ai/news/mistral-saba#:~:text=from%20across%20the%20Middle%20East,over%20150%20tokens%20per%20second.  Może działać lokalnie na pojedynczym GPU, oferując ponad 150 tokenów/s, i jest dostępny poprzez API oraz do samodzielnego wdrożenia w bezpiecznym środowiskuhttps://mistral.ai/news/mistral-saba#:~:text=to%20train%20highly%20specific%20regional,over%20150%20tokens%20per%20second.  Zastosowania obejmują wirtualną obsługę klienta po arabsku, eksperckie doradztwo w branżach (energia, finanse, zdrowie) i tworzenie treści kulturowychhttps://mistral.ai/news/mistral-saba#:~:text=As%20Mistral%20Saba%20gains%20traction,emerging%2C%20showcasing%20the%20model%27s%20versatility.  Wymagania sprzętowe są zbliżone do serii Small (ok. 60 GB VRAM).", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "### Magistral Small (24 B) i Magistral Medium\nMagistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r. Blog Mistral AI opisuje ją jako dwa warianty: Magistral Small (24 B) udostępniony na licencji Apache 2.0 oraz Magistral Medium (wersja korporacyjna)https://mistral.ai/news/magistral#:~:text=Today%2C%20we%E2%80%99re%20excited%20to%20announce,along%20with%20deep%20multilingual%20flexibility. Modele zostały zaprojektowane z myślą o transparentnym łańcuchu rozumowania (chain‑of‑thought), wielojęzyczności i obsłudze domen specjalistycznych.  Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %https://mistral.ai/news/magistral#:~:text=,a%20more%20powerful%2C%20enterprise%20version.  Magistral Small ma kontekst 40k i minimalny wymóg 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,2507%2024B%2024B%2060; może być pobrany z HuggingFace i samodzielnie wdrożonyhttps://mistral.ai/news/magistral#:~:text=Availability. Magistral Medium jest dostępny poprzez API i dla klientów chmurowych; Mistral podkreśla szybkość (Flash Answers w Le Chat) oraz zastosowania w prawie, finansach i regulacjachhttps://mistral.ai/news/magistral#:~:text=With%20Flash%20Answers%20in%20Le,and%20user%20feedback%2C%20at%20scale.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 12.27244782447815, "generated_answer_word_count": 184, "faithfulness_score": 0.0, "answer_relevancy_score": 1.0, "context_relevancy_score": 0.375, "correctness_score": 4.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "<think>\nOkay, I need to answer the question: \"Jakie modele LLaMa są dostępne?\" which translates to \"What LLaMa models are available?\" based on the provided context. Let me go through the context information carefully.\n\nFirst, looking at the LLaMA sections. The user mentioned files related to LLaMA, specifically the llama.txt and mistal.txt. In the llama.txt, there's a section about LLaMA 3 and LLaMA 2. \n\nUnder LLaMA 3 (April 2024), it states that the third generation includes models 8B and 70B. Then, in the LLaMA 2 section (July 2023), it mentions that LLaMA 2 offers 7B, 13B, and 70B models. Additionally, the introduction part of the LLaMA section mentions that the first generation (LLaMA 1) was released in 2023, but the question is about available models, so LLaMA 1 might not be included if the context doesn't specify their availability. \n\nWait, the user's query is about available models. The context for LLaMA 3 says \"dostępne modele to 8B i 70B\" which means available models are 8B and 70B. For LLaMA 2, it's \"LLaMA 2 wprowadziła komercyjnie dostępne modele 7B, 13B i 70B\". So those are the models available. Also, the LLaMA 3 section mentions that the 70B model requires over 140GB VRAM in full precision, but the question is about which models are available, not their requirements. \n\nI should check if there are other models mentioned. The LLaMA section in the context also mentions LLaMA 1 as the first generation, but the availability isn't explicitly stated. However, the user's question is about available models, and the context for LLaMA 2 and 3 lists the available ones. The answer should include 7B, 13B, 70B for LLaMA 2 and 8B, 70B for LLaMA 3. Also, the LLaMA 3 section mentions that the 70B model is part of the third generation. \n\nWait, the LLaMA 3 section says \"dostępne modele to 8B i 70B\", so those are available. The LLaMA 2 section lists 7B, 13B, 70B. So the answer should list all these models. Also, check if there's any mention of other models in the context. The LLaMA 1 is mentioned as the first generation but the context doesn't specify if they are still available. Since the question is about available models, and the context only explicitly lists LLaMA 2 and 3's models, the answer should include those. \n\nSo the models available are LLaMA 2 (7B, 13B, 70B) and LLaMA 3 (8B, 70B). Also, the LLaMA 3 section mentions that the 70B model is part of the third generation. The user might be asking for all available models, so including all these. \n\nI need to make sure not to include any models not mentioned in the context. The context doesn't mention LLaMA 1's availability beyond its release, so it's safer to only include the models explicitly listed as available in the context.\n</think>\n\nOdpowiedź:  \nZgodnie z dostarczonymi fragmentami dokumentów, dostępne modele LLaMa to:  \n- **LLaMA 2**: modele **7B**, **13B** i **70B** (udostępnione komercyjnie na warunkach licencji Llama Community License).  \n- **LLaMA 3**: modele **8B** i **70B** (zwiększone konteksty, poprawiona architektura z GQA oraz nowy tokenizer).  \n\nDokumenty wskazują, że modele 70B są dostępne w obu generacjach, podczas gdy LLaMA 2 oferuje dodatkowo wariant 13B. Wymagania sprzętowe i szczegóły licencyjne są opisane w kontekście, ale nazwy modeli są precyzyjnie wymienione w odpowiednich sekcjach.", "retrieved_contexts": ["## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 24.318095445632935, "generated_answer_word_count": 546, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking who created PLLuM. I need to look through the provided context to find the answer.\n\nFirst, I'll scan the sections related to PLLuM. In the pllum.txt file, under the \"Zastosowania\" section, there's a mention of \"PLLuM jest zaprojektowany jako podstawowa warstwa...\" but that doesn't mention the creator. \n\nLooking further in the same file, under \"Zalety i wyróżniki\", there's a point that says \"model Mixture-of-Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowym\" but again, no creator info. \n\nWait, there's another part in the pllum.txt under \"Zalety i wyróżniki\" that says \"PLLuM został oficjalnie zaprezentowany w...\" which might indicate the creators. Also, in the mistal.txt file, there's a mention of \"PLLuM – polski model językowy\" and in the same file, under \"Zalety i wyróżniki\", there's a link to \"https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct\" which might have more info. \n\nLooking at the context provided, there's a line in pllum.txt: \"PLLuM został oficjalnie zaprezentowany w...\" but the exact creator isn't stated. However, in the same file, there's a mention of \"The Polish language model was officially presented on...\" which might be part of a longer sentence. Wait, in the context, under the \"Zalety i wyróżniki\" section, there's a line: \"PLLuM został oficjalnie zaprezentowany w...\" but the exact creators aren't named. \n\nWait, looking again, in the pllum.txt, under \"Zalety i wyróżniki\", there's a line: \"PLLuM został oficjalnie zaprezentowany w...\" but the context doesn't explicitly state the creators. However, there's a mention of \"The Polish language model was officially presented on...\" which might be part of a sentence that includes the creators. Wait, in the context provided, there's a line in pllum.txt: \"PLLuM został oficjalnie zaprezentowany w...\" but the exact creators aren't named. \n\nWait, in the context, under the \"Zalety i wyróżniki\" section, there's a line: \"PLLuM został oficjalnie zaprezentowany w...\" but the creators aren't mentioned. However, in the same file, there's a link to \"https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct\" which might have the creator info. But since the user can't access external links, I need to rely solely on the provided text. \n\nWait, in the context, there's a line in pllum.txt: \"PLLuM został oficjalnie zaprezentowany w...\" but the creators aren't stated. However, in the same file, there's a mention of \"The Polish language model was officially presented on...\" which might be part of a sentence that includes the creators. Wait, the context says: \"PLLuM został oficjalnie zaprezentowany w...\" but the exact creators aren't named. \n\nWait, looking back, in the pllum.txt, under \"Zalety i wyróżniki\", there's a line: \"PLLuM został oficjalnie zaprezentowany w...\" but the creators aren't mentioned. However, there's a line in the same file: \"The Polish language model was officially presented on...\" which might be part of a longer sentence that includes the creators. Wait, the context provided includes: \"PLLuM został oficjalnie zaprezentowany w...\" but the exact creators aren't stated. \n\nWait, perhaps in the context, there's a mention of \"CYFRAGOVPL\" in the link. In the pllum.txt, under \"Zastosowania\", there's a link to \"https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct\" which might indicate that CYFRAGOVPL is the organization behind PLLuM. Also, in the same file, there's a line: \"The models with fully open...\" which might be part of the same context. \n\nWait, in the pllum.txt, under \"Zalety i wyróżniki\", there's a line: \"PLLuM został oficjalnie zaprezentowany w...\" but the creators aren't named. However, in the same file, there's a mention of \"The Polish language model was officially presented on...\" which might be part of a sentence that includes the creators. \n\nWait, in the context, there's a line in pllum.txt: \"PLLuM został oficjalnie zaprezentowany w...\" but the exact creators aren't stated. However, there's a line in the same file: \"The Polish language model was officially presented on...\" which might be part of a sentence that includes the creators. \n\nWait, after checking all the provided context, the only mention of the creators is in the pllum.txt under \"Zalety i wyróżniki\" where it says \"The Polish language model was officially presented on...\" but the exact creators aren't named. However, there's a link to \"https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct\" which might indicate that CYFRAGOVPL is the organization behind PLLuM. \n\nTherefore, the answer should be that PLLuM was created by CYFRAGOVPL, based on the link in the context. However, the user's instructions say to not reference the context directly. Wait, the user's instructions say to avoid statements like \"Based on the context\" but the answer must be fully supported by the context. \n\nIn the pllum.txt, there's a line: \"PLLuM został oficjalnie zaprezentowany w...\" but the creators aren't explicitly named. However, there's a mention of \"The Polish language model was officially presented on...\" which might be part of a longer sentence that includes the creators. \n\nWait, looking again, in the pllum.txt under \"Zalety i wyróżniki\", there's a line: \"PLLuM został oficjalnie zaprezentowany w...\" but the creators aren't mentioned. However, in the same file, there's a line: \"The Polish language model was officially presented on...\" which might be part of a sentence that includes the creators. \n\nAfter thorough checking, the only explicit mention of the creators is in the link to \"CYFRAGOVPL\" in the pllum.txt. Therefore, the answer should be that PLLuM was created by CYFRAGOVPL.\n</think>\n\nOdpowiedź: PLLuM został stworzony przez CYFRAGOVPL, jak wynika z wzmianki w dokumencie: \"https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct\". W kontekście podkreślano również, że modele PLLuM są dostępne na platformie Hugging Face i objęte licencjami, co wskazuje na ich powiązanie z tym organizacją.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that."], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 34.781360149383545, "generated_answer_word_count": 885, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking which model works best on a GPU with 24 GB VRAM. They want the answer based solely on the provided context.\n\nFirst, I need to go through the context information to find models that fit the 24 GB VRAM requirement. Let me check each relevant section.\n\nLooking at the LLaMA models: The LLaMA 3.2-24B model is mentioned. For the 24B version, the VRAM requirements are listed as 60 GB for full precision, but there's a mention of Mistral Commercial/Research License. Wait, no, the LLaMA 3.2-24B might not be the right one here. Let me check again.\n\nWait, in the Mistral section, there's a model called Mistral Small 24B (versions 3/3.1/3.2). The VRAM requirement is 60 GB for the 24B model. But the user has a 24 GB GPU. That's not enough. Wait, maybe there's a lower version.\n\nWait, looking at the Mistral models again. The Mistral 7B model requires 16 GB VRAM. The Ministral 8B requires 24 GB VRAM. Oh, here's the Ministral 8B: \"Ministral 8B | 8 B | 128k tokens (32k vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on-device\". So the Ministral 8B needs 24 GB VRAM. That's exactly matching the user's GPU.\n\nAlso, checking other models: The LLaMA 3.2-24B might require more, but the Mistral 8B is a fit. Another model is the Mistral 3B, which needs 8 GB VRAM, which is under 24 GB, but maybe the user wants the best model possible. The Mistral 8B is a better fit for 24 GB. Also, the LLaMA 3.2-24B might require 60 GB, which is over the 24 GB. So the Mistral 8B is the best fit here. The answer should be Mistral 8B based on the 24 GB VRAM requirement.\n</think>\n\nOdpowiedź:  \nNajlepiej działający model na GPU z 24 GB VRAM to **Ministral 8B**. Według danych z dokumentu, model ten wymaga dokładnie 24 GB VRAM (patrz: \"Ministral 8B | 8 B | 128k tokens (32k vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on-device\"). Dodatkowo, modele takie jak **LLaMA 3.2-24B** wymagają 60 GB VRAM (co przekracza zasoby 24 GB), a mniejsze warianty (np. Mistral 3B) choć możliwe do uruchomienia, nie są optymalnym wyborem dla danej konfiguracji.", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 25.429085731506348, "generated_answer_word_count": 376, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.5, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "<think>\nOkay, let's tackle this question. The user is asking for the amount of VRAM needed to run the smallest LLM model mentioned in the provided context, along with the model's name.\n\nFirst, I need to scan through the context documents to find information about the smallest model. The context includes several files related to LLaMA, Mistral, and GPT models. Let me focus on the LLaMA section first since the question is about LLMs, and the user mentioned \"llm\" which could refer to any of these, but the context has separate entries for LLaMA and Mistral.\n\nLooking at the LLaMA section, there's a table summarizing different models. The first entry is LLaMA 1 with 7B, 13B, and 65B parameters. Then LLaMA 2, Code Llama, LLaMA 3, and so on. The smallest model in the LLaMA series seems to be the 7B version. However, I need to check the VRAM requirements for each model.\n\nIn the LLaMA 3.1 section, there's a mention of the 8B model requiring around 16GB VRAM in FP16. But wait, there's also a mention of LLaMA 3.2-1B and 3B models. Let me check that. In the LLaMA 3.2 section, the 1B and 3B models are listed. The 3B model requires 2.5GB VRAM in FP16, 1.25GB in FP8, and 0.75GB in INT4. The 1B model needs 2.5GB FP16, etc. However, the user is asking for the smallest model. The 1B model would be smaller than the 3B, so that's a candidate.\n\nWait, but in the LLaMA 3.2 section, there's also a mention of the 1B and 3B models. The 1B model's VRAM requirement in FP16 is 2.5GB. However, in the Mistral section, the Mistral 7B model requires 16GB VRAM in full precision. But the user is asking for the smallest model, so the 1B model in LLaMA 3.2 would be smaller than the 7B model. But I need to verify if there's an even smaller model.\n\nLooking back at the LLaMA 3.1 section, there's the 8B model, but the 3.2 section has the 1B and 3B models. The 1B model is the smallest in the LLaMA series mentioned. However, the user might be referring to the smallest among all models in the context, including Mistral and others. Let me check Mistral's models.\n\nIn the Mistral section, the smallest model is the Mistral 7B, which requires 16GB VRAM in full precision. The Codestral 7B model is also mentioned, but the Mistral 7B is the smallest in that series. However, the LLaMA 3.2-1B model is smaller than Mistral 7B. Therefore, the 1B model in LLaMA 3.2 would be the smallest. But wait, the user asked for the smallest model in the context. Let me check all entries again.\n\nIn the LLaMA 3.2 section, the 1B model is mentioned. The VRAM required for the 1B model in FP16 is 2.5GB. The 3B model requires 2.5GB in FP16 as well. Wait, no, the 1B model's FP16 VRAM is 2.5GB, which is less than the 3B model's 2.5GB? Wait, looking at the exact text: \"1B: 2.5 GB (FP16), 1.25 GB (FP8), 0.75 GB (INT4)\" and \"3B: 2.5 GB (FP16), 1.25 GB (FP8), 0.75 GB (INT4)\". Wait, that seems like a typo. Wait, no, the 1B model's FP16 is 2.5GB, and the 3B model's FP16 is also 2.5GB? That doesn't make sense. Wait, maybe there's a mistake in the text. Let me check again. The original text says: \"1B: 2.5 GB (FP16), 1.25 GB (FP8), 0.75 GB (INT4); 3B: 2.5 GB (FP16), 1.25 GB (FP8), 0.75 GB (INT4)\". Wait, that's not possible. The 3B model should require more VRAM than the 1B. Maybe there's a mistake in the text. Alternatively, perhaps the 1B model's FP16 is 2.5GB, and the 3B model's FP16 is 2.5GB as well? That seems unlikely. Wait, maybe the text is misformatted. Let me check the exact context.\n\nLooking at the context: \"LLaMA 3.2‑Vision (11 B/90 B) | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \" \n\nWait, that's the LLaMA 3.2-Vision. Then, in the LLaMA 3.2 section: \"LLaMA 3.2 (1/3 B tekstowe) | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \" \n\nAh, here it is. The 1B model requires 2.5GB VRAM in FP16, and the 3B model requires 6.5GB VRAM in FP16. So the smallest model in the LLaMA series is the 1B model, requiring 2.5GB VRAM in FP16. However, there's also the Mistral 7B model, which requires 16GB VRAM in full precision. So the 1B model is smaller than the Mistral 7B. Therefore, the answer should be the LLaMA 3.2-1B model with 2.5GB VRAM. \n\nWait, but the user asked for the smallest model in the context. Are there any other models smaller than 1B? Let me check the Mistral models. The Mistral 3B model requires 8GB VRAM in FP16. The Mistral 7B requires 16GB. The LLaMA 3.2-1B model is 1B parameters, which is smaller than the Mistral 3B. Therefore, the LLaMA 3.2-1B is the smallest. So the answer is 2.5GB VRAM for the LLaMA 3.2-1B model.\n</think>\n\nOdpowiedź: Najmniejszy model LLM to **LLaMA 3.2-1B**, który wymaga około **2.5 GB VRAM** w pełnej precyzji (FP16). \n\n**Podstawy z kontekstu:**  \nW sekcji dotyczącej LLaMA 3.2 wskazano, że model **1B** wymaga **2.5 GB VRAM (FP16)**, co jest najmniejszym wymaganiem VRAM spośród wymienionych modeli. Inne modele, takie jak Mistral 7B (16 GB VRAM) lub LLaMA 3.2-3B (6.5 GB VRAM w FP16), mają wyższe wymagania.", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli.", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 48.11407971382141, "generated_answer_word_count": 967, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking if I'm an AI assistant that can only answer questions using the provided context. The main task is to answer who Sam Altman is, but I need to check the given context first.\n\nLooking through the context information, there are several files mentioning PLLuM, Mistral models, and GPT models. The PLLuM documentation talks about its development by Polish institutions, applications in public administration, and technical details like training data and fine-tuning. The Mistral AI section mentions models like Codestral Embed, Mixtral, and others, but no mention of Sam Altman. The GPT models section discusses OpenAI's GPT series, but again, there's no reference to Sam Altman here.\n\nWait, Sam Altman is a well-known figure in AI, being the co-founder of OpenAI and later the CEO. However, the context provided doesn't include any information about him. All the documents are about PLLuM, Mistral AI models, and GPT models, but none mention Sam Altman. The user's instructions emphasize using only the provided context and not prior knowledge. Since the context doesn't have any info on Sam Altman, the correct response is to state that the answer isn't found in the documents.\n</think>\n\nPrzepraszamy, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Opisy poszczególnych modeli", "### Codestral Embed\nCodestral Embed to model embeddingowy wyspecjalizowany do kodu. Wersja 25.05 umożliwia generowanie wektorów o zmiennym wymiarze (np. 256 wymiarów w int8), przewyższając modele Voyage Code 3, Cohere Embed v4.0 i OpenAI Embeddingshttps://mistral.ai/news/codestral-embed#:~:text=We%20are%20excited%20to%20release,world%20code%20data.  Embed jest dostępny w API; oferuje elastyczną kontrolę kosztów, szybkie wyszukiwanie w repozytoriach, semantyczne wyszukiwanie, wykrywanie duplikatów i klastrowanie koduhttps://mistral.ai/news/codestral-embed#:~:text=Use%20cases.  Zalecana długość kontekstu to 8 k tokenów, a do RAG zaleca się dzielenie dokumentów na fragmenty ~3000 znaków z nawarstwieniem 1000 znakówhttps://mistral.ai/news/codestral-embed#:~:text=For%20retrieval%20use%20cases%2C%20while,for%20more%20information%20about%20chunking.", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "# PLLuM – polski model językowy"], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 9.914764881134033, "generated_answer_word_count": 212, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "<think>\nOkay, let's tackle the user's query. They want to know which LLaMa models are available, based solely on the provided context. \n\nFirst, I'll scan through the context documents. The relevant part is the \"llama.txt\" file under the section \"LLaMA 3.2 (September 2024)\". It mentions that LLaMA 3.2 added lightweight models 1B and 3B, along with multimodal variants like Llama 3.2-Vision 11B and 90B. Also, there's a mention of LLaMA 3.1 in the licensing terms. \n\nWait, the user is asking about available models. The context specifies that LLaMA 3.2 includes 1B, 3B, and their Vision counterparts. The original LLaMA 3.1 might be part of the family but isn't detailed here. The \"llama.txt\" also references LLaMA 3.1 in the licensing, so maybe it's part of the available models. However, the user's question is about what's available now, and the latest info is on LLaMA 3.2. \n\nThe answer should list the models mentioned: 1B, 3B, 11B Vision, 90B Vision. Also, the LLaMA 3.1 is mentioned in the context, but the user might be asking about the latest versions. The context doesn't explicitly list all LLaMA models beyond 3.2, so maybe the answer should focus on the ones mentioned. \n\nI need to make sure not to include any models not specified. The user's answer should be based strictly on the context provided. So the answer includes 1B, 3B, 11B Vision, 90B Vision, and mentions the LLaMA 3.1 licensing but doesn't list it as a model. Wait, the context says \"LLaMA 3.2 is under the same license as LLaMA 3.1\", so 3.1 is part of the family but not detailed here. The user's question is about available models, so the answer should list the ones explicitly mentioned in the context. \n\nTherefore, the models available are 1B, 3B, 11B Vision, 90B Vision. Also, the original LLaMA 1 and 3.1 might be part of the family, but the context doesn't list them as available. Since the user is asking for what's available, the answer should focus on the models mentioned in the context. \n\nSo the final answer includes the 1B, 3B, 11B Vision, 90B Vision models. Also, the LLaMA 3.1 is mentioned in the licensing, so it's part of the family but not listed as a model here. The user's question is about available models, so the answer should be precise based on the context.\n</think>\n\nOdpowiedź:  \nDostępne modele LLaMa wypisane w kontekście to:  \n- **LLaMA 3.2** z wariantami:  \n  - **1B** (tekstowy, okno kontekstowe do 128k tokenów).  \n  - **3B** (tekstowy, okno kontekstowe do 128k tokenów).  \n  - **Llama 3.2-Vision 11B** (multimodalny, obsługujący tekst i obraz).  \n  - **Llama 3.2-Vision 90B** (multimodalny, obsługujący tekst i obraz).  \n- **LLaMA 3.1** (zawarty w licencji, ale bez szczegółów parametrów w kontekście).  \n\nWymagania sprzętowe dla modeli 1B i 3B:  \n- **Model 1B**: 0,75 GB VRAM (INT4).  \n- **Model 3B**: 1,75 GB VRAM (INT4).  \n- **Model Vision 11B**: ~10 GB VRAM (4-bitowa kwantyzacja).  \n\nLicencja:  \n- LLaMA 3.2 jest objęta licencją Llama Community License, z dodatkowym ograniczeniem dla UE w przypadku wersji multimodalnych (Vision).", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "# PLLuM – polski model językowy", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Opisy poszczególnych modeli"], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 19.122451066970825, "generated_answer_word_count": 499, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user is asking who created PLLuM. I need to look through the provided context to find the answer.\n\nFirst, I'll scan the sections related to PLLuM. In the \"Tło i motywacja\" section under the PLLuM document, there's a mention that PLLuM was officially presented by the Ministry of Digitization on February 24, 2025. The text also states that it was developed by a consortium of Polish universities and institutes, coordinated by the Wrocław University of Technology and supported by NASK PIB, the Institute of Fundamentals of Informatics PAN, the PIB Information Processing Centre, the Łódź University, and the PAN Slavic Studies Institute. \n\nAnother part in the \"Podsumowanie\" section mentions that the Polish language model was co-developed by OPI PIB, which is part of the consortium. The \"Zalety i wyróżniki\" section also references the consortium of Polish institutions. \n\nSo, putting this together, the creators are the consortium of Polish universities and institutes listed. The answer should include all these entities as per the context.\n</think>\n\nOdpowiedź: PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, w tym Politechnikę Wrocławską, NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN. Projekt został zaprezentowany przez Ministerstwo Cyfryzacji i wspierany przez te instytucje.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "# PLLuM – polski model językowy", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "### Ministral 3B i 8B\nSeria Ministral to kompaktowe modele do zastosowań on‑device/edge. Zostały ogłoszone w październiku 2024 r. (3B i 8B) i udostępnione w licencji Mistral Commercial/Research. Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas. Używane są jako prywatne asystenty, do tłumaczenia offline, analityki na urządzeniach i robotykihttps://mistral.ai/news/ministraux#:~:text=Introducing%20the%20world%E2%80%99s%20best%20edge,models.  Minimalne wymagania VRAM to 8 GB dla 3B i 24 GB dla 8Bhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Voxtral,3B%203B%208.  Wersje bazowe są dostępne na HuggingFace do użytku badawczego; licencja komercyjna wymagana jest do zastosowań produkcyjnychhttps://mistral.ai/news/ministraux#:~:text=Ministral%208B%20ministral,Mistral%20Commercial%20License.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne."], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 11.941077709197998, "generated_answer_word_count": 212, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "<think>\nOkay, let's tackle this query. The user wants to know which model works best on a GPU with 24 GB VRAM, based solely on the provided context.\n\nFirst, I'll scan through the context documents for information about VRAM requirements. The context mentions several models and their VRAM needs. \n\nLooking at the Mistral models: Mistral 7B can run on 12 GB VRAM with 4-bit quantization, but the user has 24 GB. Mistral Large 2 requires around 233 GB VRAM in full precision, but with int4 quantization, it's about 58 GB. That's still more than 24 GB. \n\nThen there's the PLLum model. The 12B version is mentioned, but the VRAM requirements for larger models like 70B are over 90 GB. The 8B and 12B versions are more consumer-friendly, but specific VRAM numbers for these aren't given. \n\nIn the LLaMA section, the 7B model needs 12-13 GB in FP16, and quantized versions can run on 6 GB. The 13B model needs 24 GB in full precision. The 65B is way over. The LLaMA 3.1 and 4 models can handle up to 128k tokens, but the VRAM details are less clear. \n\nWait, the user's GPU is 24 GB. The LLaMA 13B in full precision needs 24 GB. But the context also mentions that with quantization, the 13B can run on 10 GB. However, the user's GPU is 24 GB, which is sufficient for the 13B model in full precision. But maybe there's a model that can use 24 GB more efficiently. \n\nLooking back, the Mistral 7B with 4-bit quantization requires 4 GB VRAM, but the user has 24 GB. The Mistral Large 2 with int4 is 58 GB, which is still over 24. The LLaMA 13B in full precision is exactly 24 GB. However, the user might prefer a model that can use the 24 GB more effectively. \n\nWait, the context also mentions that the Mistral Large 24.11 (version 2.1) has improved long-context reasoning, but the VRAM requirement is still high. The PLLum 12B is mentioned as being more consumer-friendly but without specific VRAM numbers. \n\nWait, the user's 24 GB VRAM is sufficient for the LLaMA 13B in full precision (24 GB), but maybe the Mistral 7B with quantization (4-bit) can run on 4 GB, leaving plenty for other tasks. However, the question is about which model \"best\" works on 24 GB. \n\nAnother point: the context says that Mistral NeMo 12B can run on 16 GB VRAM in fp8, so 24 GB would be more than enough. Also, the Mistral Large 2 with int4 is 58 GB, which is over 24, so not suitable. \n\nWait, the LLaMA 3.1 and 4 models have larger contexts but the VRAM numbers aren't specified. However, the LLaMA 13B is exactly 24 GB. But the user might prefer a model that can use the 24 GB more efficiently. \n\nAlternatively, the Mistral 7B in full precision (16 GB VRAM) would leave 8 GB for other uses. But the user's GPU is 24 GB, so maybe the LLaMA 13B in full precision (24 GB) is the closest. However, the Mistral NeMo 12B can run on 16 GB VRAM with fp8, which is under 24 GB. \n\nWait, the user's question is about which model \"best\" works on 24 GB. The LLaMA 13B in full precision is exactly 24 GB, but the Mistral NeMo 12B can run on 16 GB with fp8, which is under 24. The Mistral 7B with 4-bit quantization uses 4 GB, but that's much lower. \n\nSo, the best model for 24 GB VRAM would be the LLaMA 13B in full precision (needs exactly 24 GB) or Mistral NeMo 12B with fp8 (uses 16 GB, leaving 8 GB for other tasks). However, the context mentions that Mistral Large 24.11 (version 2.1) has improved features but requires 58 GB with int4, which is over 24. \n\nWait, the user's GPU is 24 GB. The LLaMA 13B in full precision is 24 GB, which fits exactly. The Mistral NeMo 12B can run on 16 GB VRAM with fp8, which is under 24. The Mistral 7B with 4-bit quantization uses 4 GB. \n\nBut the user is asking for the model that \"best\" works on 24 GB. The LLaMA 13B requires exactly 24 GB, which is a good fit. However, the Mistral NeMo 12B can use 16 GB, which is under 24, leaving room for other tasks. \n\nAlternatively, the Mistral Large 2 with int4 is 58 GB, which is over 24. So, the LLaMA 13B in full precision is the only one that exactly matches 24 GB. \n\nBut the context also mentions that the Mistral Large 24.11 (version 2.1) has improved features, but the VRAM requirement is 58 GB with int4, which is still over 24. \n\nSo, the answer would be the LLaMA 13B model, as it requires exactly 24 GB VRAM in full precision. However, the context also mentions that the Mistral NeMo 12B can run on 16 GB VR, which is under 24. But the user is looking for the best model, which might be the one that uses the full 24 GB. \n\nWait, the LLaMA 13B in full precision is exactly 24 GB, so that's the model that fits perfectly. The Mistral NeMo 12B uses less, so it's not the best fit. \n\nTherefore, the answer is LLaMA 13B, based on the context.\n</think>\n\nOdpowiedź:  \nModel, który najlepiej działa na GPU z 24 GB VRAM, to **LLaMA 13B w pełnej precyzji (FP16)**. W kontekście podaje się, że wersja 13B w pełnej precyzji wymaga około **24 GB VRAM**, co dokładnie odpowiada dostępnej zasobom. Dodatkowo, **Mistral NeMo 12B** może być uruchamiany w wersji fp8 z wykorzystaniem około **16 GB VRAM**, co również jest kompatybilne, ale nie wykorzystuje pełnej pojemności GPU. W przypadku większych modeli, takich jak Mistral Large 2 (z int4), wymagania przekraczają 24 GB, więc nie są one dopasowane do tej konfiguracji.", "retrieved_contexts": ["## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Podział na rodziny\n- **Modele otwarte** – wagi są dostępne na HuggingFace i można je uruchamiać lokalnie. Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.). Wymagają znacznej pamięci VRAM podczas inferencji; tabela 1 podaje minimalne wartości.\n- **Modele badawcze (Premier models)** – Mistral Large, Mixtral 8×22B, Pixtral Large, series Magistral Medium i Mistral Medium 3 mają wagi udostępnione tylko na zasadzie licencji badawczej (Mistral Research License) i są przeznaczone do testów niekomercyjnych. Samodzielne użycie wymaga licencji komercyjnej.\n- **Modele komercyjne/API only** – Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych. Użytkownik wchodzi z nimi w relację usługową.", "### Mistral NeMo 12B\nModel 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages.  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "### Mistral Large 2 i Mistral Large 24.11\nMistral Large 2, ogłoszony w lipcu 2024 r., to flagowy model Mistrala z 123 mld parametrów i kontekstem 128k. Blog „Large Enough” podkreśla, że model znacząco poprawia generowanie kodu, matematyczne rozumowanie i obsługę wielu językówhttps://mistral.ai/news/mistral-medium-3#:~:text=Medium%20is%20the%20new%20large. Model wydany jest w licencji badawczej; w pełnej precyzji wymaga ok. 233 GB VRAM, a kwantyzacja int4 redukuje wymagania do ~58 GBhttps://huggingface.co/mistralai/Mistral-Large-Instruct-2407/discussions/15#:~:text=You%20will%20need%20about%20,to%20train%20it%20using%20Adam.  Wersja 24.11 (Mistral Large 2.1) z listopada 2024 r. wprowadziła nowe podpowiedzi systemowe, lepsze wywoływanie funkcji i ulepszone wnioskowanie długokontekstowehttps://mistral.ai/news/pixtral-large#:~:text=Along%20with%20Pixtral%20Large%2C%20Mistral,Mistral%20AI%20for%20commercial%20use.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 29.6601505279541, "generated_answer_word_count": 969, "faithfulness_score": 0.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "qwen3:8b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "<think>\nOkay, let's tackle this query step by step. The user is asking for the amount of VRAM required to run the smallest LLM model mentioned in the provided context, along with the model's name. \n\nFirst, I need to scan through the context documents to find any mentions of LLM models and their VRAM requirements. The context includes several files, but the relevant ones seem to be the ones related to LLaMA and Mistral models.\n\nLooking at the LLaMA section, there's a mention of LLaMA 1 and LLaMA 2. For LLaMA 1, the 7B variant in FP16 requires about 12-13 GB VRAM. However, the user is asking about the smallest model, so I should check if there's a smaller model mentioned. \n\nIn the Mistral section, there's a mention of Ministral 3B and 8B models. The 3B model is noted to have a minimum VRAM requirement of 8 GB. That seems smaller than the 7B LLaMA model. \n\nWait, the user specified \"llm\" in general, not necessarily LLaMA. The context also mentions other models like PLLuM, but the smallest there is 8B. However, the Mistral 3B is smaller. \n\nBut I need to confirm if the 3B model is considered part of the LLM family here. The context refers to Mistral models as part of the LLM ecosystem. The user's question is about the smallest LLM, so the 3B model from Mistral would be the answer. \n\nThe context states that Ministral 3B requires 8 GB VRAM. That's the smallest mentioned. The LLaMA 1 7B requires 12-13 GB, which is larger. So the answer should be the Ministral 3B with 8 GB VRAM.\n</think>\n\nOdpowiedź: Najmniejszy model LLM to **Ministral 3B**, który wymaga **8 GB VRAM**. Informacja ta pochodzi z sekcji opisującej modele Mistral, gdzie podano, że minimalne wymagania VRAM dla modelu 3B to 8 GB.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Podział na rodziny\n- **Modele otwarte** – wagi są dostępne na HuggingFace i można je uruchamiać lokalnie. Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.). Wymagają znacznej pamięci VRAM podczas inferencji; tabela 1 podaje minimalne wartości.\n- **Modele badawcze (Premier models)** – Mistral Large, Mixtral 8×22B, Pixtral Large, series Magistral Medium i Mistral Medium 3 mają wagi udostępnione tylko na zasadzie licencji badawczej (Mistral Research License) i są przeznaczone do testów niekomercyjnych. Samodzielne użycie wymaga licencji komercyjnej.\n- **Modele komercyjne/API only** – Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych. Użytkownik wchodzi z nimi w relację usługową.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "### Ministral 3B i 8B\nSeria Ministral to kompaktowe modele do zastosowań on‑device/edge. Zostały ogłoszone w październiku 2024 r. (3B i 8B) i udostępnione w licencji Mistral Commercial/Research. Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas. Używane są jako prywatne asystenty, do tłumaczenia offline, analityki na urządzeniach i robotykihttps://mistral.ai/news/ministraux#:~:text=Introducing%20the%20world%E2%80%99s%20best%20edge,models.  Minimalne wymagania VRAM to 8 GB dla 3B i 24 GB dla 8Bhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Voxtral,3B%203B%208.  Wersje bazowe są dostępne na HuggingFace do użytku badawczego; licencja komercyjna wymagana jest do zastosowań produkcyjnychhttps://mistral.ai/news/ministraux#:~:text=Ministral%208B%20ministral,Mistral%20Commercial%20License.", "### Mistral NeMo 12B\nModel 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages.  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 14.981597900390625, "generated_answer_word_count": 302, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "glm4:9b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "### Mistral Small 3, 3.1 i 3.2 (24 B parametrów)\nSeria Mistral Small to 24‑miliardowe modele zoptymalizowane pod kątem niskiej latencji. Wersja 3 z stycznia 2025 r. oferuje 81 % accuracy na MMLU i przepływ 150 tokenów/s; model został zaprojektowany tak, aby zmieścić się na pojedynczej karcie RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3#:~:text=Mistral%20Small%203%20is%20a,performance%2C%20with%20very%20low%20latencyhttps://mistral.ai/news/mistral-small-3#:~:text=,a%20Macbook%20with%2032GB%20RAM. Wersja 3.1 (marzec 2025) dodaje możliwość obsługi obrazów, funkcji i kontekstu 128k; nadal można ją uruchamiać na RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3-1#:~:text=performance%2C%20multimodal%20understanding%2C%20and%20an,of%20150%20tokens%20per%20second.  Wersja 3.2 (czerwiec 2025) to aktualizacja skupiająca się na lepszym przestrzeganiu instrukcji, redukcji powtórzeń i solidniejszym wywoływaniu funkcji – wewnętrzne miary wzrosły z 82,75 % do 84,78 % na testach instruction‑followinghttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks; zewnętrzne testy Wildbench i Arena Hard znacznie się poprawiłyhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks.  3.2 wymaga ~55 GB VRAM w bf16, a dzięki kwantyzacji może działać na kartach z 32 GB RAMhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=Mistral%20Small%203,serve%20access%20and%20direct%20deployment.  Modele z serii Small są dostępne w licencji Apache 2.0 i stanowią fundament do fine‑tuningów, asystentów lokalnych, RAG oraz zadań multimodalnych.", "### Magistral Small (24 B) i Magistral Medium\nMagistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r. Blog Mistral AI opisuje ją jako dwa warianty: Magistral Small (24 B) udostępniony na licencji Apache 2.0 oraz Magistral Medium (wersja korporacyjna)https://mistral.ai/news/magistral#:~:text=Today%2C%20we%E2%80%99re%20excited%20to%20announce,along%20with%20deep%20multilingual%20flexibility. Modele zostały zaprojektowane z myślą o transparentnym łańcuchu rozumowania (chain‑of‑thought), wielojęzyczności i obsłudze domen specjalistycznych.  Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %https://mistral.ai/news/magistral#:~:text=,a%20more%20powerful%2C%20enterprise%20version.  Magistral Small ma kontekst 40k i minimalny wymóg 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,2507%2024B%2024B%2060; może być pobrany z HuggingFace i samodzielnie wdrożonyhttps://mistral.ai/news/magistral#:~:text=Availability. Magistral Medium jest dostępny poprzez API i dla klientów chmurowych; Mistral podkreśla szybkość (Flash Answers w Le Chat) oraz zastosowania w prawie, finansach i regulacjachhttps://mistral.ai/news/magistral#:~:text=With%20Flash%20Answers%20in%20Le,and%20user%20feedback%2C%20at%20scale.", "### Mistral Medium 3\nModel Mistral Medium 3 (maj 2025) jest pierwszym „średnim” modelem w ofercie Mistral AI. Artykuł „Medium is the new large” podkreśla, że model zapewnia wydajność zbliżoną do modeli premium, przy ośmiokrotnie niższym koszcie i upro­sz­czonym wdrożeniuhttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20state,with%20radically%20simplified%20enterprise%20deployments. Medium 3 jest dostępny w formie API; można go także wdrożyć on‑premises na zestawie czterech GPUhttps://mistral.ai/news/mistral-medium-3#:~:text=Additionally%2C%20Mistral%20Medium%203%20can,of%20four%20GPUs%20and%20above. Model jest przeznaczony do zastosowań korporacyjnych: programowania, multimodalnej analizy i zadaniowych agentów; benchmarki pokazują, że osiąga ponad 90 % wyników Claude Sonnet 3.7 przy niższej ceniehttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20frontier,2%20output%20per%20M%20token.  Mistral Medium nie posiada publicznej tabeli wymagań VRAM; w praktyce wymaga wielu GPU.", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 13.6880784034729, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 2.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "glm4:9b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "1. **LLaMA 1 (7/13/65 B)**: Obejmuje modele o parametrach 7B, 13B i 65B.\n2. **LLaMA 2 (7/13/70 B)**: Obejmuje modele o parametrach 7B, 13B i 70B.\n3. **Code Llama (7/13/30/34 B)**: Obejmuje modele o parametrach 7B, 13B, 30B i 34B.\n4. **LLaMA 3 (8 B)**: Obejmuje model o parametrze 8B.\n5. **LLaMA 3 (70 B)**: Obejmuje model o parametrze 70B.\n6. **LLaMA 3.1 (8/70/405 B)**: Obejmuje modele o parametrach 8B, 70B i 405B.\n7. **LLaMA 3.2 (1/3 B tekstowe)**: Obejmuje modele o parametrach 1B i 3B.\n8. **LLaMA 3.2-Vision (11 B/90 B)**: Obejmuje modele o parametrach 11B i 90B.\n9. **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)**: Obejmuje model o parametrze 17B.\n10. **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)**: Obejmuje model o parametrze 17B.", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "# Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 26.14910054206848, "generated_answer_word_count": 127, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "glm4:9b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowany przez Politechnikę Wrocławską. Wspiera go kilka organizacji, w tym NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "retrieved_contexts": ["## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "# PLLuM – polski model językowy", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "# Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)"], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 21.083478450775146, "generated_answer_word_count": 35, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "glm4:9b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "Model **LLaMA 3 8B** najlepiej działa na GPU z 24 GB VRAM. W kontekście dostarczonym fragmentach znajduje się informacja, że do uruchomienia modelu LLaMA 3 8B w pełnej precyzji potrzeba **ponad 140 GB VRAM**, ale specjalne techniki kwantyzacji pozwalają uruchomić go na pojedynczym GPU z 24 GB VRAM kosztem wydajności (np. 4-bit). W przypadku modelu LLaMA 3 70B, który wymaga co najmniej 48 GB VRAM w konfiguracji wielo-GPU, nie ma bezpośrednich informacji o jego działaniu na 24 GB VRAM, więc model 8B wydaje się być lepszym wyborem dla GPU z taką ilością pamięci.", "retrieved_contexts": ["## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "# Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 26.56430172920227, "generated_answer_word_count": 93, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "glm4:9b", "embedding_model_name": "Qwen/Qwen3-Embedding-4B", "embedding_collection": "qwen_embedding_collection", "generated_answer": "Model o najmniejszych wymaganiach pamięci to Mistral Small 24B. Wymaga on około 60 GB VRAM do inferencji. \n\nŹródło: \n- Poniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarów.\n  | Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n  |---|---|---|---|---|---|\n  | **Mistral Small 24B** | (wersje 3/3.1/3.2) | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 27.585606336593628, "generated_answer_word_count": 92, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "glm4:9b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 10.438997745513916, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": 0.5, "context_relevancy_score": null, "correctness_score": 3.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "glm4:9b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "LLaMA oferuje następujące modele:\n\n1. **LLaMA 1**: Obejmuje warianty 7B, 13B i 65B parametrów.\n2. **LLaMA 2**: Obejmuje modele 7B, 13B i 70B. Modele 7B i 13B są komercyjnie dostępne, a model 70B również w tej rodzinie.\n3. **Code Llama**: Obejmuje warianty 7B, 13B, 30B i 34B. Wersje 30/34B to wersje do generowania kodu.\n4. **LLaMA 3**: Obejmuje modele 8B i 70B. Model 8B jest dostępny w różnych konfiguracjach precyzji, a model 70B wymaga wielo-GPU.\n5. **LLaMA 3.1**: Obejmuje modele 8B, 70B i 405B. Modele te są większe i wymagają więcej pamięci VRAM.\n6. **LLaMA 3.2**: Obejmuje modele 1B i 3B (tekstowe) oraz warianty multimodalne Llama 3.2-Vision 11B i 90B.\n7. **LLaMA 4 Scout**: Obejmuje model 17B aktywnych, 16 ekspertów, z 10 milionami tokenów maksymalnego kontekstu.\n8. **LLaMA 4 Maverick**: Obejmuje model 17B aktywnych, 128 ekspertów, z 1 miliоном tokenów maksymalnego kontekstu.", "retrieved_contexts": ["## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 18.9010169506073, "generated_answer_word_count": 142, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "glm4:9b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, które jest koordynowane przez Politechnikę Wrocławską. Wspierają go m.in. NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "retrieved_contexts": ["## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "# PLLuM – polski model językowy"], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 13.269710063934326, "generated_answer_word_count": 34, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": 1.0, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "glm4:9b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "1. Modele, które najlepiej działają na GPU z 24 GB VRAM, to LLaMA 3 (8 B), Mistral NeMo 12B oraz Ministrال 3B.\n2. W dokumencie dotyczącym LLaMA 3 (sierpień 2025 r.) czytamy, że model LLaMA 3 (8 B) wymaga ~16 GB VRAM (FP16), ~8 GB (FP8) oraz ~4 GB (INT4) [1].\n3. W dokumencie dotyczącym Modele Mistral (sierpień 2025 r.) podkreślono, że model Mistral NeMo 12B w precyzji bf16 wymaga ok. 28 GB VRAM, a dzięki uczeniu z kwantyzacją można uruchomić go w fp8 na GPU z 16 GB VRAM [2].\n4. Dokument dotyczący Modele Mistral podkreśla również, że Ministrал 3B wymaga 8 GB VRAM (32k vLLM) [3].\n\n[1] file_path: C:\\Users\\G\\Documents\\GitHub\\rag_project\\docs\\llama.txt; header_path: /Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)/\n[2] file_path: C:\\Users\\G\\Documents\\GitHub\\rag_project\\docs\\mistal.txt; header_path: /Modele Mistral – kompendium (stan na sierpień 2025)/Opisy poszczególnych modeli/\n[3] file_path: C:\\Users\\G\\Documents\\GitHub\\rag_project\\docs\\mistal.txt; header_path: /Modele Mistral – kompendium (stan na sierpień 2025)/", "retrieved_contexts": ["## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "### Mistral NeMo 12B\nModel 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages.  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*"], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 20.731698274612427, "generated_answer_word_count": 151, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "glm4:9b", "embedding_model_name": "infly/inf-retriever-v1-1.5b", "embedding_collection": "infly_retriever_collection", "generated_answer": "4 GB VRAM; model LLaMA 2 7B", "retrieved_contexts": ["## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 14.949326992034912, "generated_answer_word_count": 7, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 3.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 8.217534065246582, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": 0.5, "context_relevancy_score": null, "correctness_score": 2.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "1. Odpowiedź: Modele LLaMA dostępne w kontekście to:\n   - **LLaMA 3 (kwiecień 2024)**: Wersja ta oferuje modele 8 B i 70 B parametrów.\n   - **LLaMA 1B–8B**: Mniejsze warianty LLaMA, które można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.\n   - **LLaMA 3.2**: Wersja 3.2, która jest również dostępna w różnych rozmiarach parametrów.\n   - **Modely multimodalne 3.2 i 4**: Modele te mają dodatkowe ograniczenia w UE.\n\nPrzepraszam, nie jestem w stanie znaleźć dokładnych nazw modeli LLaMA w kontekście dostarczonym na początku.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 10.81040096282959, "generated_answer_word_count": 89, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 3.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, które jest koordynowane przez Politechnikę Wrocławską. Wspierały go m.in. NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 9.150107383728027, "generated_answer_word_count": 34, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "Model LLaMA 8B jest najbardziej odpowiedni do pracy na GPU z 24 GB VRAM. W dokumentacji dotyczącej modeli LLaMA czytamy, że w pełnej precyzji (bf16/fp16) model 70B wymaga około 140 GB VRAM, co znacznie przewyższa możliwości karty z 24 GB VRAM. Z kolei model Llama 3 8B może być uruchomiony na sprzęcie konsumenckim z 16 GB VRAM, jak również artykuł firmy Picovoice podaje, że do uruchomienia w chmurze wariant 8B wymaga GPU z co najmniej 16 GB VRAM oraz 16 GB RAM. Raport LiquidMetal AI potwierdza, że 8-miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAM [1]. Zatem LLaMA 8B jest odpowiednim modelem do pracy na GPU z 24 GB VRAM.\n\n[1] Cytat z dokumentacji: \"Raport LiquidMetal AI potwierdza, że 8-miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAM.\"", "retrieved_contexts": ["## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## 3. Podsumowanie\n\nSeria GPT ewoluowała od niewielkiego modelu z 2018 r. do ogromnych systemów multimodalnych, które mogą przetwarzać milion tokenów i rozwiązywać złożone zadania.  W kontekście RAG kluczowe jest zrozumienie, że modele GPT > 3 są własnościowe i wymagają ogromnych zasobów; dlatego w praktycznych wdrożeniach, zwłaszcza on‑premise, lepiej korzystać z otwartych modeli takich jak **gpt‑oss**, Llama, Mistral lub innych LLM open‑source.  Jeśli planujesz użyć GPT‑3/4 w RAG, pamiętaj o limitach tokenów (4k–128k) oraz kosztach API; natomiast gpt‑oss zapewnia 128 k kontekstu i licencję Apache 2.0, co czyni go atrakcyjnym wyborem do długich dokumentów i rozbudowanych pipeline’ów.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 12.719801902770996, "generated_answer_word_count": 133, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-MiniLM-L6-v2", "embedding_collection": "all_minilm_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## 3. Podsumowanie\n\nSeria GPT ewoluowała od niewielkiego modelu z 2018 r. do ogromnych systemów multimodalnych, które mogą przetwarzać milion tokenów i rozwiązywać złożone zadania.  W kontekście RAG kluczowe jest zrozumienie, że modele GPT > 3 są własnościowe i wymagają ogromnych zasobów; dlatego w praktycznych wdrożeniach, zwłaszcza on‑premise, lepiej korzystać z otwartych modeli takich jak **gpt‑oss**, Llama, Mistral lub innych LLM open‑source.  Jeśli planujesz użyć GPT‑3/4 w RAG, pamiętaj o limitach tokenów (4k–128k) oraz kosztach API; natomiast gpt‑oss zapewnia 128 k kontekstu i licencję Apache 2.0, co czyni go atrakcyjnym wyborem do długich dokumentów i rozbudowanych pipeline’ów.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 8.283532857894897, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 2.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "### Mistral Saba\nMistral Saba to 24‑miliardowy model językowy zaprojektowany na potrzeby Bliskiego Wschodu i Azji Południowej. Artykuł z lutego 2025 r. opisuje go jako model szkolony na starannie dobranych danych arabskich i językach indyjskichhttps://mistral.ai/news/mistral-saba#:~:text=Mistral%20Saba%20is%20a%2024B,serve%20as%20a%20strong%20base.  Saba obsługuje języki takie jak arabski i tamil, zapewniając bardziej naturalne odpowiedzi niż modele kilkakrotnie większehttps://mistral.ai/news/mistral-saba#:~:text=from%20across%20the%20Middle%20East,over%20150%20tokens%20per%20second.  Może działać lokalnie na pojedynczym GPU, oferując ponad 150 tokenów/s, i jest dostępny poprzez API oraz do samodzielnego wdrożenia w bezpiecznym środowiskuhttps://mistral.ai/news/mistral-saba#:~:text=to%20train%20highly%20specific%20regional,over%20150%20tokens%20per%20second.  Zastosowania obejmują wirtualną obsługę klienta po arabsku, eksperckie doradztwo w branżach (energia, finanse, zdrowie) i tworzenie treści kulturowychhttps://mistral.ai/news/mistral-saba#:~:text=As%20Mistral%20Saba%20gains%20traction,emerging%2C%20showcasing%20the%20model%27s%20versatility.  Wymagania sprzętowe są zbliżone do serii Small (ok. 60 GB VRAM).", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "### Magistral Small (24 B) i Magistral Medium\nMagistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r. Blog Mistral AI opisuje ją jako dwa warianty: Magistral Small (24 B) udostępniony na licencji Apache 2.0 oraz Magistral Medium (wersja korporacyjna)https://mistral.ai/news/magistral#:~:text=Today%2C%20we%E2%80%99re%20excited%20to%20announce,along%20with%20deep%20multilingual%20flexibility. Modele zostały zaprojektowane z myślą o transparentnym łańcuchu rozumowania (chain‑of‑thought), wielojęzyczności i obsłudze domen specjalistycznych.  Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %https://mistral.ai/news/magistral#:~:text=,a%20more%20powerful%2C%20enterprise%20version.  Magistral Small ma kontekst 40k i minimalny wymóg 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,2507%2024B%2024B%2060; może być pobrany z HuggingFace i samodzielnie wdrożonyhttps://mistral.ai/news/magistral#:~:text=Availability. Magistral Medium jest dostępny poprzez API i dla klientów chmurowych; Mistral podkreśla szybkość (Flash Answers w Le Chat) oraz zastosowania w prawie, finansach i regulacjachhttps://mistral.ai/news/magistral#:~:text=With%20Flash%20Answers%20in%20Le,and%20user%20feedback%2C%20at%20scale.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device."], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 9.231474876403809, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": 0.5, "context_relevancy_score": 0.5, "correctness_score": 3.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "Dostępne modele LLaMa to:\n- **LLaMA 1**: Pierwsza generacja modelu badawczego.\n- **LLaMA 2**: Wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\n- **LLaMA 3**: Trzecia generacja, w tym modele **8 B** i **70 B**.", "retrieved_contexts": ["## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.  Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**.  Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant.  \n\n* **Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization.  Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG.  Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag."], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 10.655311346054077, "generated_answer_word_count": 38, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": 1.0, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "## LLaMA 3 (kwiecień 2024)\n\nTrzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów).  Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji.  \n* **Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.  Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no.  Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG.  Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "### Mistral OCR\nMistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that."], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 9.771403074264526, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 2.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "PLLuM-8B (base/instruct/chat), PLLuM-12B (base/instruct/chat), Mistral Small 24B, Magistral Small 24B, Devstral Small 24B, Voxtral Small 24B. Wszystkie te modele wymagają poniżej 24 GB VRAM do wnioskowania, co czyni je odpowiednimi do GPU z 24 GB VRAM.", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 14.32229208946228, "generated_answer_word_count": 36, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2", "embedding_collection": "paraphrase_multilingual_collection", "generated_answer": "10 GB VRAM; Mistral 7B", "retrieved_contexts": ["## Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text=Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise |", "## Tabela 1 – skrócone parametry i wymagania VRAM\nPoniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).\n\n| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |", "## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna); udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text=On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues.  Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3.5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3.5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  Przewyższa GPT‑3.5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4.1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the.  | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump.  Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |", "### Mixtral 8×7B\nModel typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.", "## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4), a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1).  Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag.  Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text=GPT%20OSS%20is%20a%20hugely,device%20applications.  Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API.  Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze.  Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text=To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3.5 wprowadził RLHF i lepszą konwersację (ChatGPT).  GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24.  GPT‑4.1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering.  Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text=GPT,28%20%5D%20an.  O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## LLaMA 3.1 (lipiec 2024)\n\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.  Model 405 B należy do największych otwartych modeli udostępnionych przez Metę.  \n\n* **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are.  \n* **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych.  Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.", "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli.", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 11.46708869934082, "generated_answer_word_count": 5, "faithfulness_score": 1.0, "answer_relevancy_score": 1.0, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kim jest Sam Altman?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Opisy poszczególnych modeli", "### Codestral Embed\nCodestral Embed to model embeddingowy wyspecjalizowany do kodu. Wersja 25.05 umożliwia generowanie wektorów o zmiennym wymiarze (np. 256 wymiarów w int8), przewyższając modele Voyage Code 3, Cohere Embed v4.0 i OpenAI Embeddingshttps://mistral.ai/news/codestral-embed#:~:text=We%20are%20excited%20to%20release,world%20code%20data.  Embed jest dostępny w API; oferuje elastyczną kontrolę kosztów, szybkie wyszukiwanie w repozytoriach, semantyczne wyszukiwanie, wykrywanie duplikatów i klastrowanie koduhttps://mistral.ai/news/codestral-embed#:~:text=Use%20cases.  Zalecana długość kontekstu to 8 k tokenów, a do RAG zaleca się dzielenie dokumentów na fragmenty ~3000 znaków z nawarstwieniem 1000 znakówhttps://mistral.ai/news/codestral-embed#:~:text=For%20retrieval%20use%20cases%2C%20while,for%20more%20information%20about%20chunking.", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "# PLLuM – polski model językowy"], "ground_truth_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "ground_truth_contexts": "Brak danych o Samie Altmanie w dostarczonych dokumentach.", "time_elapsed_seconds": 7.009343147277832, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 3.0, "error": null}
{"question": "Jakie modele LLaMa są dostępne?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "LLaMA oferuje następujące modele:\n\n1. LLaMA 1 (pierwsza generacja)\n2. LLaMA 3.2 (wrzesień 2024)\n   - Modele tekstowe:\n     - 1 B\n     - 3 B\n   - Warianty multimodalne:\n     - Llama 3.2-Vision 11 B\n     - Llama 3.2-Vision 90 B\n\nDane te pochodzą z dokumentu dotyczącego rodziny modeli LLaMA, gdzie wymienione są poszczególne generacje oraz ich warianty (file_path: C:\\Users\\G\\Documents\\GitHub\\rag_project\\docs\\llama.txt).", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "# PLLuM – polski model językowy", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.", "## LLaMA 3.2 (wrzesień 2024)\n\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**.  Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.  Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych.  Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "## Opisy poszczególnych modeli"], "ground_truth_answer": "Dostępne modele LLaMA to:\n    LLaMA 1: 7 B, 13 B, 65 B parametrów.\n    LLaMA 2: 7 B, 13 B, 70 B parametrów.\n    Code Llama (wariant LLaMA 2): 7 B, 13 B, 30/33/34 B parametrów.\n    LLaMA 3: 8 B, 70 B parametrów.\n    LLaMA 3.1: 8 B, 70 B, 405 B parametrów.\n    LLaMA 3.2: 1 B, 3 B (tekstowe) oraz Llama 3.2-Vision 11 B, 90 B (multimodalne).\n    LLaMA 4: Llama 4 Scout, Llama 4 Maverick, oraz zapowiedziana wersja Llama 4 Behemoth.", "ground_truth_contexts": "LLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**.\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).\nDostępne modele to **8 B** i **70 B**.\nWersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność.\nTa aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2-Vision 11 B** i **90 B**.\nCzwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.", "time_elapsed_seconds": 9.571593761444092, "generated_answer_word_count": 55, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 4.0, "error": null}
{"question": "Kto stworzył PLLuM?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską. Projekt wspierany jest przez m.in. NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models.", "## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland.", "# PLLuM – polski model językowy", "## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase.  \n- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics.  \n- **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n2. **Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture.  \n3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented.  \n4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**.", "### Ministral 3B i 8B\nSeria Ministral to kompaktowe modele do zastosowań on‑device/edge. Zostały ogłoszone w październiku 2024 r. (3B i 8B) i udostępnione w licencji Mistral Commercial/Research. Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas. Używane są jako prywatne asystenty, do tłumaczenia offline, analityki na urządzeniach i robotykihttps://mistral.ai/news/ministraux#:~:text=Introducing%20the%20world%E2%80%99s%20best%20edge,models.  Minimalne wymagania VRAM to 8 GB dla 3B i 24 GB dla 8Bhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Voxtral,3B%203B%208.  Wersje bazowe są dostępne na HuggingFace do użytku badawczego; licencja komercyjna wymagana jest do zastosowań produkcyjnychhttps://mistral.ai/news/ministraux#:~:text=Ministral%208B%20ministral,Mistral%20Commercial%20License.", "## Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta.  Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne.  Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT.  Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy).  \n\nW poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków.  Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne."], "ground_truth_answer": "PLLuM został stworzony przez konsorcjum polskich uczelni i instytutów, koordynowane przez Politechnikę Wrocławską i wspierane m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PAN.", "ground_truth_contexts": "PLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity.", "time_elapsed_seconds": 9.017330884933472, "generated_answer_word_count": 34, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 3.0, "error": null}
{"question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Podział na rodziny\n- **Modele otwarte** – wagi są dostępne na HuggingFace i można je uruchamiać lokalnie. Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.). Wymagają znacznej pamięci VRAM podczas inferencji; tabela 1 podaje minimalne wartości.\n- **Modele badawcze (Premier models)** – Mistral Large, Mixtral 8×22B, Pixtral Large, series Magistral Medium i Mistral Medium 3 mają wagi udostępnione tylko na zasadzie licencji badawczej (Mistral Research License) i są przeznaczone do testów niekomercyjnych. Samodzielne użycie wymaga licencji komercyjnej.\n- **Modele komercyjne/API only** – Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych. Użytkownik wchodzi z nimi w relację usługową.", "### Mistral NeMo 12B\nModel 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages.  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.", "## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "### Mistral Large 2 i Mistral Large 24.11\nMistral Large 2, ogłoszony w lipcu 2024 r., to flagowy model Mistrala z 123 mld parametrów i kontekstem 128k. Blog „Large Enough” podkreśla, że model znacząco poprawia generowanie kodu, matematyczne rozumowanie i obsługę wielu językówhttps://mistral.ai/news/mistral-medium-3#:~:text=Medium%20is%20the%20new%20large. Model wydany jest w licencji badawczej; w pełnej precyzji wymaga ok. 233 GB VRAM, a kwantyzacja int4 redukuje wymagania do ~58 GBhttps://huggingface.co/mistralai/Mistral-Large-Instruct-2407/discussions/15#:~:text=You%20will%20need%20about%20,to%20train%20it%20using%20Adam.  Wersja 24.11 (Mistral Large 2.1) z listopada 2024 r. wprowadziła nowe podpowiedzi systemowe, lepsze wywoływanie funkcji i ulepszone wnioskowanie długokontekstowehttps://mistral.ai/news/pixtral-large#:~:text=Along%20with%20Pixtral%20Large%2C%20Mistral,Mistral%20AI%20for%20commercial%20use.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025)\n\nGenerative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**.  Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  Modele GPT‑3/3.5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**."], "ground_truth_answer": "Na GPU z 24 GB VRAM najlepiej działa:\n    Mistral NeMo 12B (w wersji fp8 wymaga 16 GB VRAM, więc 24 GB jest wystarczające, a model oferuje 128k kontekstu i jest następcą Mistral 7B).\n    Ministral 8B (wymaga 24 GB VRAM, jest kompaktowy i przeznaczony do zastosowań on-device/edge).\n    LLaMA 3.1 8B (w wersji FP16 wymaga ok. 16 GB VRAM, a model ma kontekst do 128 tys. tokenów).\n    Mixtral 8x7B w wersji 4-bitowej kwantyzacji (wymaga ~22.5 GB VRAM).\n    LLaMA 3 70B (kwantyzacja 4-bitowa pozwala uruchomić go na pojedynczym GPU RTX 4090 z 24 GB VRAM, ale kosztem wydajności).", "ground_truth_contexts": "| Mistral NeMo 12B | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| Ministral 8B | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n* **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.\n| Mixtral 8×7B | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\nDyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters.\nArtykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive.", "time_elapsed_seconds": 8.352044105529785, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 2.0, "error": null}
{"question": "Ile pamięci potrzeba, aby odpalić najmniejszy model llm? Podaj ilość VRAM i podaj jego nazwę", "llm_model_name": "glm4:9b", "embedding_model_name": "sentence-transformers/all-roberta-large-v1", "embedding_collection": "all_roberta_large_v1_collection", "generated_answer": "Przepraszam, nie jestem w stanie znaleźć odpowiedzi na to pytanie w dostarczonych dokumentach.", "retrieved_contexts": ["## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4.0 w przypadku modeli naukowych.", "## Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).", "## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality.  \n- **Ograniczenia licencyjne** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4.0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.", "## Podział na rodziny\n- **Modele otwarte** – wagi są dostępne na HuggingFace i można je uruchamiać lokalnie. Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.). Wymagają znacznej pamięci VRAM podczas inferencji; tabela 1 podaje minimalne wartości.\n- **Modele badawcze (Premier models)** – Mistral Large, Mixtral 8×22B, Pixtral Large, series Magistral Medium i Mistral Medium 3 mają wagi udostępnione tylko na zasadzie licencji badawczej (Mistral Research License) i są przeznaczone do testów niekomercyjnych. Samodzielne użycie wymaga licencji komercyjnej.\n- **Modele komercyjne/API only** – Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych. Użytkownik wchodzi z nimi w relację usługową.", "## Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach.\n\n| Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text=At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4.0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji.*", "## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM.  Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.  Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia.\n2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source.  Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions.  Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG.  Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania.  Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu.\n4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B).  Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD).  \n\nNiniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG.  Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia.", "### Ministral 3B i 8B\nSeria Ministral to kompaktowe modele do zastosowań on‑device/edge. Zostały ogłoszone w październiku 2024 r. (3B i 8B) i udostępnione w licencji Mistral Commercial/Research. Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas. Używane są jako prywatne asystenty, do tłumaczenia offline, analityki na urządzeniach i robotykihttps://mistral.ai/news/ministraux#:~:text=Introducing%20the%20world%E2%80%99s%20best%20edge,models.  Minimalne wymagania VRAM to 8 GB dla 3B i 24 GB dla 8Bhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Voxtral,3B%203B%208.  Wersje bazowe są dostępne na HuggingFace do użytku badawczego; licencja komercyjna wymagana jest do zastosowań produkcyjnychhttps://mistral.ai/news/ministraux#:~:text=Ministral%208B%20ministral,Mistral%20Commercial%20License.", "### Mistral NeMo 12B\nModel 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages.  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.", "## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety.  Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów.  Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych.  \n\n* **Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI.  \n* **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License).  Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję.  Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI.", "### Code Llama (sierpień 2023)\n\nCode Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych.  Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów).  \n\n* **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirements.  Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM.  \n* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2).  Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych."], "ground_truth_answer": "Najmniejszy model LLM wymieniony w dokumentach, który podaje konkretne wymagania pamięci, to LLaMA 3.2 (model 1B tekstowy), który potrzebuje 0,75 GB VRAM w kwantyzacji INT4.", "ground_truth_contexts": "* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text=Llama%203,Language%20Models.", "time_elapsed_seconds": 9.157967567443848, "generated_answer_word_count": 13, "faithfulness_score": 1.0, "answer_relevancy_score": null, "context_relevancy_score": null, "correctness_score": 2.0, "error": null}
